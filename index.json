[{"content":" Standalone plots Here you see some of my data visualizations exclusively rendered in matplotlib without additional edits from graphical editors. Most of them are from my Masters Thesis.\nConference posters The following list includes some posters from University projects as well as conference presentations. All posters are exclusively rendered in LaTex.\nTitle Presented Link Complex frequency modulations in freely interacting electric fish recorded in their natural habitat International Conference of Neuroethology, Lisbon 2022 pdfgithub Automated detection of chirps in electric fish: Deep learning applied to ethology Annual Meeting of the Ethological Society, Muenster 2024 pdf Data videos These were created by producing series of individual frames with Matplotlib and combining them into videos with ffmpeg.\n","date":"19 May 2024","externalUrl":null,"permalink":"/projects/dataviz/","section":"Data Analysis Projects","summary":"A gallery of recent data visualizations, from simple plots to conference posters and even a video.","title":"A collection of data visualizations","type":"projects"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/creativecoding/","section":"Tags","summary":"","title":"Creativecoding","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/datascience/","section":"Tags","summary":"","title":"Datascience","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/datavis/","section":"Tags","summary":"","title":"Datavis","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/dataviz/","section":"Tags","summary":"","title":"Dataviz","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/design/","section":"Tags","summary":"","title":"Design","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/matplotlib/","section":"Tags","summary":"","title":"Matplotlib","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/categories/post/","section":"Categories","summary":"","title":"Post","type":"categories"},{"content":"Random walks are a beautifully simple concept that many beginners encounter in their coding journey. These basic algorithms can be extended to create stunning visualizations. Below, you\u0026rsquo;ll see how enhancing a simple random walk can result in a captivating display:\nA standard random walk typically restricts movement to four cardinal directions. However, by allowing the walk to step in any angular direction, we can create far more dynamic patterns. I first explored this idea during my master\u0026rsquo;s thesis on simulating the movement of electric fish, and later found inspiration in a Reddit post that employed a similar concept.\nThe real beauty of this approach lies not just in the freedom provided by the unit circle but also in how we can purposefully limit this freedom. If an agent were to step in a completely random direction at each juncture, the path would appear highly chaotic. Typically, \u0026ldquo;real\u0026rdquo; agents (whatever they might represent) move in relatively straight trajectories most of the time.\nTo achieve more natural movement, rather than selecting a direction at random each time, we can utilize a probability density function (PDF) to influence the likelihood of choosing certain directions based on the previous step\u0026rsquo;s direction. In this example, we\u0026rsquo;ll use a Gaussian PDF, although for simulating the motions of a Knifefish, I employed a bimodal Gaussian or a von Mises distribution to mimic the characteristic forward and backward movement of these fish.\nSuch configurations ensure that the trajectory of the subsequent step remains close to the previous step, with variations governed by the standard deviation of the Gaussian.\nLet\u0026rsquo;s jump into the code. First, we need to import some libraries and set up Matplotlib to use a dark theme.\nimport numpy as np import matplotlib.pyplot as plt import numpy as np import seaborn as sns from matplotlib.collections import LineCollection plt.rcParams.update( { \u0026#34;figure.facecolor\u0026#34;: \u0026#34;black\u0026#34;, \u0026#34;axes.facecolor\u0026#34;: \u0026#34;black\u0026#34;, \u0026#34;axes.edgecolor\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;axes.labelcolor\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;xtick.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;ytick.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;grid.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;text.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;legend.facecolor\u0026#34;: \u0026#34;black\u0026#34;, \u0026#34;legend.edgecolor\u0026#34;: \u0026#34;white\u0026#34;, } ) Now lets declare some parameters. Playing with them will substantially impact the output of our random walk.\nn_walkers = 500 n_steps = 500 starting_point = (0, 0) The initial_seed will be the possible trajectories that the first step of each walker is drawn from.\ninitial_seeds = np.linspace(0, 2 * np.pi, n_walkers) The gaussian_pdf_sigmas will be the standard deviations of the probability density functions that determine the the variability of the trajectory in the next step, given the current step.\ngaussian_pdf_sigmas = np.linspace(np.pi / 150, np.pi / 100, n_walkers) Let\u0026rsquo;s also select a colormap and introduce a circle, on which the random walkers start from.\ncmap = sns.color_palette(\u0026#34;mako\u0026#34;, as_cmap=True) circle_radius = 220 # Adjustable radius for the circle Now let\u0026rsquo;s randomly draw the initial trajectories for each walker as well as the standard deviation that determines the variability of each walker.\nseeds = np.random.choice(initial_seeds, n_walkers) pdf_sigmas = np.random.choice(gaussian_pdf_sigmas) The following lines set the initial position of each random walker onto the boundary of a circle that we can control with the circle_radius parameter.\n# Compute starting positions on the rim of the circle start_x = circle_radius * np.cos(seeds) start_y = circle_radius * np.sin(seeds) # Initialize positions array to store positions at each step for each walker positions = np.zeros((n_walkers, n_steps, 2)) positions[:, 0, 0] = start_x positions[:, 0, 1] = start_y The main loop for the random walk is straightforward but compelling in its simplicity. We are adjusting the direction slightly using a Gaussian PDF while updating the walker\u0026rsquo;s position on each step. This could probably be optimized to run faster, but at this point this is not nessecary and I think this is much more readable.\n# Perform the random walk for step in range(1, n_steps): # Generate directions based on Gaussian distribution seeds = np.random.normal(seeds, pdf_sigmas) # Calculate step increments dx = np.cos(seeds) dy = np.sin(seeds) # Update positions positions[:, step, 0] = positions[:, step - 1, 0] + dx positions[:, step, 1] = positions[:, step - 1, 1] + dy For the visualization, we differentiate the paths by their distance from the origin, adding an aesthetic dimension to the display.\n# Plot the paths of the walkers fig, ax = plt.subplots(figsize=(20, 20)) for i in range(n_walkers): # get x and y positions x = positions[i, :, 0] y = positions[i, :, 1] # compute distance to origin at (0,0) dist = np.sqrt(x**2 + y**2) points = np.array([x, y]).T.reshape(-1, 1, 2) segments = np.concatenate([points[:-1], points[1:]], axis=1) # Create a continuous norm to map from data points to colors norm = plt.Normalize(dist.min(), dist.max()) lc = LineCollection(segments, cmap=cmap, norm=norm) # Set the values used for colormapping lc.set_array(dist) lc.set_linewidth(1.5) lc.set_alpha(1) line = ax.add_collection(lc) ax.axis(\u0026#34;equal\u0026#34;) ax.axis(\u0026#34;off\u0026#34;) # plt.savefig(\u0026#34;cover1.jpg\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;, pad_inches=0, dpi=300) plt.show() I hope you find these visual results as fascinating as I do. With further adaptation, such as integrating principles from the Boids algorithm — which emphasizes coherence, separation, and alignment — we could guide the walkers into forming dynamic flocks as they evolve. This concept is something that we may explore in a future post.\n","date":"19 May 2024","externalUrl":null,"permalink":"/posts/angular_randomwalk/","section":"Posts","summary":"Let\u0026rsquo;s see how basic random walks can be extended a bit to create interesting visuals.","title":"Pretty pictures with angular random walks","type":"posts"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/categories/projects/","section":"Categories","summary":"","title":"Projects","type":"categories"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/randomwalk/","section":"Tags","summary":"","title":"Randomwalk","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/simulation/","section":"Tags","summary":"","title":"Simulation","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Masters student in the Neuroethology Group @ UniTue.\nInterested in quantitative Animal Behavior \u0026amp; Data Science.\nLearn more! ","date":"19 May 2024","externalUrl":null,"permalink":"/","section":"Welcome","summary":"Masters student in the Neuroethology Group @ UniTue.","title":"Welcome","type":"page"},{"content":"","date":"18 May 2024","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"We’ve all been there: you find an exciting coding experiment, quickly copy the code, try to execute it, and it fails. Not because of missing dependencies, but due to missing functions, variables, or messed-up code order. I hate that.\nAs a somebody who values simplicity and efficiency, I’ve often struggled with tools like Jupyter Notebooks. They offer robust features but can complicate version control, file management, and require heavy IDEs. As a Neovim enthusiast, I prefer plain text solutions. This led me to a novel approach:\nWhat if you could write your entire article in a simple Python file, using comment blocks for text and converting it to Markdown?\nThis method simplifies writing, keeps everything in a single file, and integrates perfectly with version control. Plus, it ensures that the file is executable and easy to run, making it a self-contained and efficient solution.\nHere\u0026rsquo;s the plan:\nUse top-level block comments (enclosed by triple quotes) for text. Treat the rest as code. A program to achieve this needs to:\nExtract text from top-level block comments. Wrap Python code in Markdown code blocks. Let\u0026rsquo;s explore how to do this using standard Python libraries.\nImports and helper functions We\u0026rsquo;ll employ dataclasses to manage our content blocks, argparse to handle command line inputs for source and destination files and re for some regex magic. Ready to revolutionize your writing process? Let’s get started!\nfrom dataclasses import dataclass import argparse import re As we have to keep the order of the content intact, my approach essentially structures the content of the file into blocks. Each block can either be a content or a code block. So first, lets define a simple dataclass that will hold data of each block, witch is really only its content and its category:\n@dataclass class Block: content: str is_code: bool As a next step, lets define a function that helps us parse the command line arguments. The only thing we need right now is the location of a Python file that you would like to test this with and the location to where the output file should go.\ndef argparser(): parser = argparse.ArgumentParser( description=\u0026#34;Convert python files into markdown\u0026#34;, ) parser.add_argument( \u0026#34;--python\u0026#34;, \u0026#34;-p\u0026#34;, type=str, help=\u0026#34;Path to the python file.\u0026#34;, ) parser.add_argument( \u0026#34;--markdown\u0026#34;, \u0026#34;-m\u0026#34;, type=str, help=\u0026#34;Path to the output file.\u0026#34;, ) args = parser.parse_args() return args We also need a small utility function that checks if the current line is the line that indicates the beginning or end of a comment or not:\ndef is_comment(line): return line.startswith(\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;) or line.startswith(\u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;) And a small function that strips the comment syntax from the line.\ndef remove_comment_syntax(line): if line.startswith(\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;): line = line.replace(\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;) elif line.startswith(\u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;): line = line.replace(\u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) elif line.endswith(\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;): line = line.replace(\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;) elif line.endswith(\u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;): line = line.replace(\u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) return line Extract blocks of code and text Now lets write the main functionality: We will parse the file line by line. As soon as we encounter the start or stop of a comment, we flip a boolean switch that keeps track of whether we are currently inside of a comment block or inside of a code block. As soon as we detect the start of a code comment block, we start to collect all the lines of code into the comment_block list. If we encounter the stop of the comment, we concatenate them into a single string in store them, in addition tho the information of the block type, into an instance of the Block class, wich we collect in our blocks list. The same applies to the code blocks. We need to add an additional last if-statement at the end: because of the logic I used to parse the file, the function \u0026ldquo;assumes\u0026rdquo; that the last block is always a comment. If, after the loop is finished, we still have content in our code_block list, we concatenate this as well and store it in a block.\ndef extract_comments(file_path): with open(file_path, \u0026#34;r\u0026#34;) as file: code_lines = file.readlines() blocks = [] in_comment_block = False comment_block = [] code_block = [] for i, line in enumerate(code_lines): # if this line marks the start or end of a comment block ... if is_comment(line): # ... and we are not already in a comment block, enter it if not in_comment_block: if i != 0: # only append the code block if we are not in the first line block = Block( content=\u0026#34;\u0026#34;.join(code_block), is_code=True, ) blocks.append(block) code_block = [] comment_block.append(remove_comment_syntax(line)) in_comment_block = True # ... and we are already in comment block, this is the end so we # exit it else: block = Block( content=\u0026#34;\u0026#34;.join(comment_block), is_code=False, ) blocks.append(block) comment_block = [] in_comment_block = False # if this is not the start or stop of a comment ... else: # ... but we currently are in a comment block, add the text to the # markdown list if in_comment_block: comment_block.append(line) # ... but we are not currently in a comment, then this is code else: code_block.append(line) # handle the end when the last block is not a comment but a code block if len(code_block) \u0026gt; 0: block = Block( content=\u0026#34;\u0026#34;.join(code_block), is_code=True, ) blocks.append(block) code_block = [] return blocks This is probably not the most elegant implementation of this algorithm but I whipped it together quickly and for now it works well. The output of this function is now a list of instances of our Block class, each containing the block content and whether it is code or content (i.e., Markdown text). What we now need, is a function that puts this together to a Markdown file. For this, we can simply iterate over our blocks and wrap each block into a Markdown code block, if it contains code. Easy as that!\ndef build_markdown(blocks, path): file = \u0026#34;\u0026#34; for i, block in enumerate(blocks): if len(block.content) == 0: continue # wrap the code in markdown code blocks if block.is_code: file += \u0026#34;\\n```python\\n\u0026#34; file += block.content file += \u0026#34;\\n```\\n\u0026#34; elif not block.is_code: file += block.content # remove more that 3 consecutive line breaks file = re.sub(r\u0026#34;\\n{3,}\u0026#34;, \u0026#34;\\n\\n\u0026#34;, file) with open(path, \u0026#34;w\u0026#34;) as mdfile: mdfile.write(file) Putting it all together Now let us add a main function that ties this all together: It first parses the arguments, then runs the function that extracts the blocks of text and code and then puts it all back together into a Markdown file.\ndef main(): args = argparser() blocks = extract_comments(args.python) build_markdown(blocks, args.markdown) Adding this piece of code now executes the main function if the script is called directly.\nif __name__ == \u0026#34;__main__\u0026#34;: main() You can now call this script from your terminal on any Python file. In fact, the article you are currently reading was produced by the very Python code you\u0026rsquo;ve just seen. You can check out the full content on my GitHub. To use the script, save it as py2md.py and run it on itself with the following command:\npython3 py2md.py -p py2md.py -m out.md Alternatively, you can install it as a package directly from GitHub with the following command.\npip install git+github.com/weygoldt/py2md Then you can use the script from your terminal directly. For example, to generate the package README.md, I simply called from within the repository:\npy2md -p py2md/main.py -m README.md This will generate the article you are reading now, formatted as a Markdown file. Naturally, this method works with other Python files as well.\nConclusion By leveraging the simplicity of plain Python files and the power of basic Python libraries, you can streamline your writing process and maintain full control over versioning. This method not only simplifies the integration of code and text but also ensures that your articles remain easy to manage and most importantly, reproduce. This is particularly important for data-centric projects, where being able to generate your report from the terminal, complete with all plots and without errors, is immensely satisfying. Happy writing!\n","date":"18 May 2024","externalUrl":null,"permalink":"/posts/py2md/","section":"Posts","summary":"How to convert inline a Python file with some inline block comments to a beautiful markdown document","title":"How to write coding articles in plain Python files","type":"posts"},{"content":"","date":"18 May 2024","externalUrl":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"Markdown","type":"tags"},{"content":"","date":"18 May 2024","externalUrl":null,"permalink":"/tags/reproducibility/","section":"Tags","summary":"","title":"Reproducibility","type":"tags"},{"content":"","date":"18 May 2024","externalUrl":null,"permalink":"/tags/writing/","section":"Tags","summary":"","title":"Writing","type":"tags"},{"content":"Understanding the significance of specific communication cues necessitates our ability to detect them, particularly with transient frequency modulation signals like chirps produced by an electric organ discharge in electric fish. Previous research has mainly focused on immobilizing or artificially stimulating fish or physically separating them, conditions unfavorable for natural communication.\nTo address this challenge, I designed a convolutional neural network-based detector capable of detecting chirps in freely behaving fish. Despite initially training the model on simulated data, it surprisingly performed well on real-world recordings after some fine-tuning. Using this version, I successfully detected approximately 50,000 chirps, marking the largest dataset at that time.\nThe following image illustrates a short segment of a recording featuring two fish. Chirps are visible as frequency fluctuations from the baseline of one of the two fish on a spectrogram. The dots indicate where the detector identified a chirp.\nA spectrogram of a recording of two fish. The dots indicate where the detector detected a chirp. Performing preliminary analyses utilizing this detector, we discovered that chirps could potentially be utilized by the losing fish to indicate submission during competition for a shelter among two fish.\nweygoldt/cnn-chirpdetector A first try of detecting the transient communication signals of weakly electric fish on a spectrogram using a convolutional neural network. Python 0 0 ","date":"18 November 2023","externalUrl":null,"permalink":"/projects/chirp_detector/","section":"Data Analysis Projects","summary":"Electric fish produce fast frequency sweeps to communicate, called chirps. Here, I detected these chirps with a CNN-classifier on spectrograms that was trained on data I simulated.","title":"Chirp detector","type":"projects"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/cnn/","section":"Tags","summary":"","title":"Cnn","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/dsp/","section":"Tags","summary":"","title":"Dsp","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/efish/","section":"Tags","summary":"","title":"Efish","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/neuralnetwork/","section":"Tags","summary":"","title":"Neuralnetwork","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/spectrogram/","section":"Tags","summary":"","title":"Spectrogram","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/timeseries/","section":"Tags","summary":"","title":"Timeseries","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer-Vision","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/dataset/","section":"Tags","summary":"","title":"Dataset","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep-Learning","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/label-studio/","section":"Tags","summary":"","title":"Label-Studio","type":"tags"},{"content":"Note: This guide was written using label-studio version 1.8.2.post1.\nWith the modules included in pytorch, opencv or ultralytics, training computer vision models is easier than ever, the main constraint being the availability of labeled data. While there are many \u0026ldquo;smart\u0026rdquo; labeling solutions, most of them are either expensive or not open source. Label-studio is unique in that it free to use, open source and even can be made \u0026ldquo;smart\u0026rdquo; by adding your own pre-labeling network for automatic labeling.\nBut setting up label-studio locally to correct pre-annotated images of my YOLO dataset that I passed through a Faster R-CNN model trained on synthetic data took me about two days. So I decided to write this in case I forget how I did it. And maybe it helps someone else before they waste two days of their life.\n1. Convert the dataset to label studio format First of all, you need a dataset in the YOLO format, which is structured like this:\ndataset ├── classes.txt ├── images │ ├── image1.png │ ├── image2.png │ └── ... └── labels ├── image1.txt ├── image2.txt └── ... Label studio cannot directly import a YOLO dataset that is labeled. It could import the images only, but for the labels to be loaded as well, we need to convert them to the label-studio json format.\nThis can be achieved by the label-studio-converter tool, which comes with label studio.\nThe converter needs 4 parameters:\n--input: The path to the dataset, i.e. in our case that would be /home/user/data/dataset\n--output: The path to the output file, i.e. /home/user/data/dataset/output.json. To make life easier, just put it into the dataset root directory.\n--image-root-url: Now this is the tricky one: The root URL is now a relative path. Don\u0026rsquo;t ask me why, but it needs to be relative to the parent of the root URL exported as our environment variable (as you will see later). And to make life even harder, it needs to be prefixed with /data/local-files/?d=. So in our case, the root URL would be /data/local-files/?d=dataset/images. Label studio will serve the images from the root URL and the images are located in the images folder of the dataset.\n--image-ext: The file extension of the images. In our case, that would be .png.\nAnd to assemble the full command:\nlabel-studio-converter import yolo -i /home/user/data/dataset -o /home/user/data/dataset/output.json --image-root-url \u0026#34;/data/local-files/?d=dataset/images\u0026#34; --image-ext .png This will write two files into the dataset directory: output.json and output.label_config.xml. The latter one contains the label format that we need to import the dataset into label studio.\n2. Export environment variables To label local images, label-studio needs to be launched with two environment variables set. The first one enables local file serving and the second one sets the document root.\nThe document root must be an absolute path and the direct parent of the dataset directory. Example: If the root URL is /home/user/data then the dataset, itself containing an image and label folder, must reside in e.g. /home/user/data/dataset.\nexport LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true export LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/home/user/data To make this easier, I put this into a small bash script that exports the variables and launches label studio:\n#!/bin/bash export LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true export LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/home/user/data label-studio Just do a chmod +x on the script and you can start label studio with ./label-studio.sh.\n3. Create a new project in label studio Open label-studio and create a new project.\nGive your project some name. Paste the content of the output.label_config.xml file into the label config field. Go to the settings and add a Cloud Storage, choosing Local Storage as the type. Set the storage root URL as an absolute path to the root of your dataset, i.e. /home/user/data/dataset. Now, import the output.json file into the project. You should now have the labels and images loaded into label studio.\n4. Label the images Your project should now be ready to correct the labels. You can now start labeling the images. Once you are done, you can export the labels using the export button back into the YOLO format. This will produce a zip file containing the images and the labels. Unzip it and you are back where you started just with hopefully better labels.\nI hope this helps someone. If you have any questions, feel free to contact me.\n","date":"10 November 2023","externalUrl":null,"permalink":"/posts/label_studio_yolo/","section":"Posts","summary":"Note: This guide was written using label-studio version 1.","title":"Labeling images with Label Studio","type":"posts"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/supervised-learning/","section":"Tags","summary":"","title":"Supervised-Learning","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/yolo/","section":"Tags","summary":"","title":"Yolo","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/2023/","section":"Tags","summary":"","title":"2023","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/amapa/","section":"Tags","summary":"","title":"Amapa","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/brazil/","section":"Tags","summary":"","title":"Brazil","type":"tags"},{"content":"Recently, I had the privilege of joining Jan Benda on a research expedition to dive head first into the realm of electric fish in the Amazonian Rainforest.\nOver the course of nine days at a remote field station, we were immersed in the challenges and wonders of this unique ecosystem, all in pursuit of unraveling the mysteries of these aquatic creatures.\nA Pictorial Journey Let\u0026rsquo;s start with some visuals to set the scene:\nWhy Electric Fish? In the vast expanse of the Neotropics, electric fish are ubiquitous. While most people have heard of the electric eel, there are numerous lesser-known species that are small, nocturnal, and rarely observed without specialized equipment. These \u0026lsquo;weakly\u0026rsquo; electric fish utilize their electric organs for communication, navigation, and hunting in the Amazonian Rainforest waters. While they serve as a popular model in Neurophysiology, their ecological roles, natural behaviors, and communication methods remain largely enigmatic. This is where our journey begins.\nGymnorhamphichthys, a pulse-type weakly electric fish that inhabits the sand banks of Amazonian rivers. The Research Objective Wave-type electric fish, comprising many species, are unique in that they constantly emit an electric field. We can measure this field using electrodes and amplifiers. The field of each fish oscillates at a unique frequency, allowing us to distinguish them. Our aim is to take this a step further: employing electrode grids to measure the electric field on a plane and reconstruct the movements, communication, and even the mating behaviors of multiple individuals in their natural habitat. While similar work has been done before, we aim to (1) enhance the resolution from the previous 64 to 256 electrodes and (2) make the system portable, modular, and robust. During this field trip, we tested the first prototype of this system.\nThe Expedition to the Field Station Reaching our field station was no easy feat. We embarked on a journey that took us from Frankfurt to Lisbon and then to Belem, a city in northern Brazil. From Belem, we hopped on a small plane to Macapa, the capital of the state of Amapa. From there, we were graciously driven to Porto Grande by Christoph Jaster, head of the local park administration, where we met our boatsman, Junior, who would transport us to the field station. After two hours in the car and another two hours on the river, we arrived at the southernmost tip of the FLONA do Amapa, a national park nestled within the Amazonian Rainforest.\nThe field station, a small house covered by the dense canopy of the Amazonian Rainforest. The field station itself, perched at the confluence of the Rio Falsino and the Rio Araguari, was a humble abode with a small kitchen, bathrooms, and space for hammocks. In the evenings, a generator powered the lights, and we even had satellite internet. However, most of the time, we were serenaded by the sounds of the rainforest, the chirping of birds, and the distant howling of monkeys.\nA Day in the Life at the Field Station Our days began with the early rising sun at around 6 AM. After a quick breakfast, the mornings were devoted to setting up, fixing, or debugging our equipment. In the afternoons, Junior would ferry us to the river for fieldwork, where we tested our recording hardware.\nJan and me taking a first look at new data on our favorite riverbank. Navigating the river\u0026rsquo;s shallows and rapids was exhilarating, especially since our visit coincided with the dry season, causing the water levels to drop noticeably. Despite the challenges, Junior skillfully guided our boat, and we never found ourselves stranded or pushing the boat through shallow waters. Maneuvering through the river\u0026rsquo;s rapids proved to be an enjoyable part of our daily routine.\nOn the lookout for big rocks beneath the surface. The sweltering midday heat often left us lethargic, prompting us to seek respite in our hammocks for a brief siesta or to \u0026ldquo;cool off\u0026rdquo; in the 32°C river waters. These moments also offered the opportunity to wash and dry our clothes in the sun within a few hours.\nMost evenings, we returned from the river with new challenges to tackle, leading us to spend the nights fixing and debugging our equipment. Telma, our cook, prepared meals for us and provided much-needed relief from cooking duties. After dinner, our evenings were dedicated to data analysis and equipment maintenance. Falling asleep in our sweaty hammocks, enveloped by the sounds of the rainforest, was a welcome reprieve after a long day\u0026rsquo;s work.\nEquipment and Data Collection Our expedition was equipped with two main categories of hardware:\nFish Finders At its simplest, a fish finder can be an audio amplifier. However, instead of plugging in a guitar or microphone, we connected our electrodes. This allowed us to audibly detect the electric organ discharges of the fish. While mere audio signals sufficed for locating the fish, Jan is in the process of developing a Smart Fish Finder with a screen capable of plotting fish waveforms. This advancement will enable us to discern the genus of the fish without visual confirmation.\nJan using his fish finder to record a Gymnorhamphichthys in the sand. Recording Hardware Once we identified promising recording locations, we deployed our logging devices. Our goal is to create a platform of affordable, waterproof loggers constructed from readily available components. The most exciting aspect? These loggers are modular and adaptable for various recording purposes:\nLogger Configuration Each logger consists of a microcontroller, a battery, an SD card, and currently, a temperature sensor. It can receive input from up to 16 electrodes, with recording duration limited only by the SD card\u0026rsquo;s capacity and battery life. In theory, they could record data for months, and deploying a network of loggers over a large area could capture migratory patterns of fish. This is the configuration we tested the most on during our field trip.\nGrid Configuration Our plan is to employ these same loggers, with the addition of a cable for power, arranged side by side to form an electrode grid. With 16 electrodes, the placement of 16 loggers would result in a 256-electrode grid. This setup would enable us to record the electric field on a plane, facilitating the reconstruction of interactions and communication among multiple individuals. However, after five days of rigorous testing and debugging, we discovered that the current circuit boards introduced interference when devices were placed closely together. Thus, we will need to redesign and retest the circuit boards in the future.\nWildlife and Biodiversity In addition to our electric fish research, our time in the rainforest afforded us the opportunity to observe a myriad of wildlife. We encountered a variety of avian species, including toucans, parrots, macaws, hummingbirds, and more. Our journey also brought us in contact with numerous insects, captivating moths and butterflies, reptiles, and amphibians.\nMost of the fish we encountered were small catfish, as the electric fish we sought tend to hide in crevices during the day. Nevertheless, we did have a few remarkable encounters, including Eigenmannia at night and an Archolaemus, which I managed to capture on video with my phone as it nestled between two rocks. This endeavor resulted in an amusing photo of me attempting to capture a good shot of the fish.\nObtaining a video was a source of great joy, given the elusive nature of these fish. Communicating about them using only data points on a screen can be challenging, so having a video was invaluable.\nWe also encountered a specimen of Gymnorhamphichthys, a pulse-type weakly electric fish that buries itself in the sand on riverbanks. Without our electrodes, locating them would have been exceedingly difficult, but with their help, we pinpointed two of them and even recorded a video of one. The background noise you hear is the electric organ discharge, which our amplifiers convert into sound.\nHowever, one electric fish appeared to shadow us wherever we ventured: Electrophorus, the electric eel. On our first day in the field, as we explored a small tributary of the Rio Falsino, we encountered four eels, each about 1.5 meters in length, serenely lounging in shallow water.\nIn the ensuing days, similar scenes greeted us each time we approached areas where smaller streams met the main channel. We also detected them (not visually, but via our fish finders) while working in the middle of the river. Picture standing chest-deep in the middle of a river, immersed in your work, and suddenly hearing the distinct \u0026rsquo;toc, toc, toc\u0026rsquo; of an approaching electric eel. When we heard them on the amplifiers, we knew they were just meters away, prompting us to swiftly exit the water and wait for them to pass. We also learned that they frequently traversed the swimming area at the station, as we had recorded them there before. Over time, we stopped carrying the fish finders when we wanted to relax and cool off, leaving the door open for future underwater encounters.\nI must confess that I had underestimated these creatures. I once naively assumed them to be \u0026lsquo;simple,\u0026rsquo; solitary hunters that relied on their electric organ to stun prey. However, as we were fortunate to observe, they are highly social animals, with documented instances of group hunting. I am eager to see what we can uncover about them in the future.\nAn electric eel passing by our recording electrodes on a tributary of the Rio Falsino. Challenges and Lessons Learned Our expedition was not without its share of challenges, some expected and others not. Yet, we garnered valuable lessons and insights from them, fueling our enthusiasm to return to the field and test our new equipment.\nThe most significant lesson we took away was that no matter how extensively you test your equipment in the lab, field conditions will inevitably present new issues. Paradoxically, these challenges are opportunities to enhance the robustness of your equipment. Hence, this field trip was indispensable, as it exposed problems we would not have encountered in a controlled environment. We are elated to have embarked on this journey and eager to return.\nData Analysis and the Road Ahead Currently, we are just starting analyzing the data we gathered during this expedition. Our exploration is just beginning, and we are thrilled about the potential discoveries that await us. We will keep you updated on our progress. In the meantime, here are some preliminary findings:\nThe first figure displays the pulse waveform of one of the Gymnorhamphichthys we recorded. The top panel shows the electric organ discharge recording, while the bottom panel illustrates the waveform of the pulses.\nThe second figure showcases a recording from one of our loggers, revealing at least four individuals, with three in the lower frequency range, potentially Sternopygi. The fourth individual operates in the higher frequency range.\nAcknowledgements First and foremost, I would like to express my deepest gratitude to Jan Benda for extending the invitation to join him on this remarkable field trip. It was an invaluable opportunity to work with him and learn from his expertise.\nThis field trip would not have been possible without the generous assistance of many individuals. The logistical challenges of such an expedition were more significant than we anticipated, and we are incredibly thankful for the support we received.\nDavid de Santana played a pivotal role in connecting us with Christoph Jaster. Without this connection, we would have remained unaware of the existence of the FLONA do Amapa field station. We are sincerely appreciative of his contribution.\nChristoph Jaster diligently managed the permits and logistics from Macapa to the field station. His assistance was indispensable, particularly when our equipment was lost in transit. Furthermore, his kindness was evident when he drove us to the airport at 1 AM when our departure was imminent.\nOur heartfelt thanks go to Telma, our cook at the field station. Her delicious meals and unwavering kindness were a tremendous relief, sparing us the task of preparing our own food. We are deeply grateful for her support.\nJunior proved to be an exceptional boatsman. Communicating through gestures, a few words, and Google Translate, we not only enjoyed our time with him but also spent a memorable night under the stars in our hammocks along the Araguari River bank. We hope to have the privilege of working with him again in\nGroup picture of the overnighter on the river bank. From left to right: Me, Jan, and Junior. ","date":"7 November 2023","externalUrl":null,"permalink":"/posts/brasil_2023/","section":"Posts","summary":"A small report on a field trip to the Amazonian Rainforest in Amapa, Brazil in 2023","title":"Exploring Electric Fish in the Rainforest of Brazil","type":"posts"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/field-trip/","section":"Tags","summary":"","title":"Field Trip","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/rainforest/","section":"Tags","summary":"","title":"Rainforest","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/research/","section":"Tags","summary":"","title":"Research","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/science/","section":"Tags","summary":"","title":"Science","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-analysis/","section":"Tags","summary":"","title":"Data Analysis","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"Data Engineering","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-science-stack/","section":"Tags","summary":"","title":"Data Science Stack","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-visualization/","section":"Tags","summary":"","title":"Data Visualization","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"The Python ecosystem is huge and it is growing every day. It is hard to keep up with all the new libraries and tools. In this post I will share my tech stack for data science in 2023. I will update this post every year to document my plunge into this never ending rabbit hole.\nDisclaimer: Everything below is my opinion. If I get something wrong, please let me know. I don\u0026rsquo;t use all of these tools in every project and I don\u0026rsquo;t list all tools that I use here. I am merely providing a list of tools that I particularly enjoy using and that I think are worth checking out as I am writing this post.\nWhy Python? Pythion is a general purpose programming language that is used for a wide variety of applications. It is a great language for data science because it is easy to learn, has a huge ecosystem of libraries and tools and it is free and open source. And beyond data science, it can be used for almost anything else.\nTerminal and shell The terminal is where I spend half of my working day. Ideally, it should be fast, functional and pretty. And most importantly, it should be simple. For this reason, I switched from Alacritty to Kitty:\nKitty - Kitty is a fast, featureful, GPU based terminal emulator. It is fast, has great support for unicode and is highly customizable. It is my go to terminal emulator. It is also cross platform and works on Windows, Linux and macOS. Compared to Alacritty, it requires a minimal amount of configuration and is easy to set up. It comes with sane keybindings out of the box and even has build-in support for ligatures, emojis, images and even tiling! No need to configure tmux. The same goes for the shell:\nZsh - As opposed to Bash, Zsh is a shell designed for interactive use. It is a great shell with a huge ecosystem of plugins and themes. I use it with starship as a prompt and vim keybindings. This way I always now where I am, which environment I am in and what git branch I am on. Text editor The text editor is where I spend the other half of my working day. After trying a lot of different editors, I have settled on two editors that I use for different purposes:\nVisual Studio Code - Visual Studio Code is a free code editor made by Microsoft for Windows, Linux and macOS. It is a bit slow but a breeze to set up and configure and settings sync between devices. It has a huge ecosystem of extensions and is my go to editor for almost everything. Because it is made by Microsoft, it integrates perfectly with GitHub Copilot, a GPT-4 powered AI that helps to make repetitive tasks easier. It is a great tool for writing code.\nNeovim - Neovim is a fork of Vim aiming to improve user experience and plugins. It is a bit harder to set up and configure but it is blazing fast and has a huge ecosystem of plugins. I use it for quick edits and for editing files on remote servers. I used it exclusively for a while but eventually broke my config during a time when I was too busy to fix it.\nNow that we can write code, we need to set up a virtual environment to run it. In my opinion, virtual environments are a must for any Python project. They allow you to create isolated environments for each project and make sure that the correct versions of your dependencies are installed. This is especially important for data science projects because they often require specific versions of Python and its dependencies. It also prevents your system from getting cluttered with dependencies that you don\u0026rsquo;t need. And since the environment can be frozen and shared with others, it makes your project reproducible.\nManging a Python environment Let\u0026rsquo;s say your system uses Python 3.9 but the package you want to use requires Python 3.11. This is where pyenv comes in:\nPyenv - Pyenv lets you easily switch between multiple versions of Python, just like Anaconda but without Anaconda! It is a great tool for managing Python versions and virtual environments. For smaller projects, pyenv-virtualenv is a great tool that lets you create virtual environments that are all stored in one place. For larger projects, I prefer pythons build in venv module. If you have used Pythons build-in venv module, you know that it is a bit of a hassle to always source .venv/bin/activate. This is where direnv comes in:\nDirenv - Direnv is an environment switcher for the shell. It knows how to hook into bash, zsh, etc. to load or unload environment variables depending on the current directory. This is a great tool for managing environment variables and secrets. I use it to automatically activate and deactivate virtual environments. Pyenv can do this out of the box but only for pyenv virtual environments. Direnv can do this for any virtual environment. Now that we have an environment, we can start setting up a project.\nManaging a Python project Version control is integral to any software project. It allows you to keep track of changes to your code and makes it easy to collaborate with others. This is where git and GitHub come in:\nGit - Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. It is the de facto standard for version control and is a must for any software project.\nGithub - GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.\nNow that we set up a git repository, we can install a pre-commit hook, that runs every time we push changes to the remote repository:\nPre-commit - Pre-commit is a framework for managing and maintaining pre-commit hooks. Pre-commit hooks are scripts that run before a commit is made. They can be used to check for formatting, linting, security issues, etc. I use it to check for formatting, linting and security issues. It is a great tool for maintaining code quality: If a pre-commit hook fails, the commit is aborted and the error is shown to the developer so that it can be fixed. This keeps remote repositories clean and makes sure that all code is formatted, linted and secure. The next tool is a great addition to pre-commit: Keeping the code clean Black - Black is the uncompromising Python code formatter. It is a great tool for keeping code clean and consistent. isort - A tool to sort package imports consistently. ruff - The fastest python formatter that also sorts imports, this is my current go-to tool. Pylint - Pylint is a tool that checks for errors in Python code, tries to enforce a coding standard. Pytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. If you build a Python package, the next one is particularly useful:\nPoetry - Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. It is a great tool for managing dependencies and packaging because it locks versions of your depencencies to their exact commit hash. Combined with the correct Python version and virtual environment, this makes your project reproducible, which is a must, particularly for data science projects. Data science libraries Now that we have set up a project, we can start writing code. The following libraries are the ones that I use most often to analyze data, train machine learning models and visualize results:\nNumpy - NumPy is the fundamental package for scientific computing in Python. It is a great library for working with arrays and matrices. It is the foundation of the scientific Python ecosystem and is used by almost every other library.\nPandas - Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. It is a great library for working with tabular data.\nScipy - SciPy is a Python-based ecosystem of open-source software for mathematics, science, and engineering. It is a great library for scientific computing and includes algorithms for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and many more.\nMatplotlib - Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. It is a great library for data visualization. It provides fine grained control over every aspect of a figure and is highly customizable.\nPydantic - Pydantic is a library for data validation and settings management based on Python type hints. It is a great library for validating data and settings. I use it to create data models for my data science projects.\nRich - Rich is a Python library for rich text and beautiful formatting in the terminal. It is a much nicer way to display logs, data and progress bars compared to e.g. tqdm. It is also the progress bar that pip uses when you install something.\nMachine learning tools Scikit-learn - Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN. It is a great library for machine learning.\nPytorch - PyTorch is an open source library for deep learning and GPU accelerated computing. It is pain-free to install compared to TensorFlow and has a great API.\nTensorflow - Another deep learning framework that comes with Keras, a high level API that makes the usage of pretrained models as well as the creation of coustom models very fast.\nThe following set of packages are training frameworks, that mostly build on top of pytorch to abstract a lot of the boiler plate code and make model training and evalutation easier.\nFastAI - The library is a bit dated but the website provides amazing courses on deep learning. Keras - A high level API that works with Tensorflow, Pytorch and JAX. Pytorch Lightning - This is framework to make pytorch models as easy as fitting sklearn models. SuperGradients - The new kid on the block by DECI-AI. Home of the current state of the art object detection model YOLO-NAS. If a project gets bigger and other people start using it, the following tools are great for documentation:\nDocumenting and sharing Jupyter - Jupyter is a free, open-source, interactive web tool known as a computational notebook, which researchers can use to combine software code, computational output, explanatory text and multimedia resources in a single document. Still, I try to avoid coding in Jupyter notebooks as much as possible but it is a great tool for sharing and documenting exploratory data analysis.\nMkdocs - MkDocs is a fast, simple and downright gorgeous static site generator that\u0026rsquo;s geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. MkDocs builds completely static HTML sites that you can host on GitHub pages, Amazon S3, or anywhere else you choose.\nPdoc - Like mkdocs, pdoc is a documentation generator for Python modules. Unlike mkdocs, it is specifically designed to be used with docstrings in Python code: pdoc automatically generates documentation from type annotations and docstrings. It is also much faster than mkdocs and is best used for API documentation.\nReporting LaTeX - LaTeX is a typesetting system that includes features designed for the production of technical and scientific documentation. It is my favourite tool for writing reports, posters and presentations. It is a bit hard to learn but once you get the hang of it, it is a breeze to use. And the best part is that it is all in plain text so it works great with version control. Conclusion As this is the first version of this post, It does include some tools that are obvious choices for data science in python. But I still think they are worth mentioning because they are so foundational. As I update this post every year, I will remove tools that are no longer relevant and add new tools that I think are worth checking out. If you have any suggestions, please let me know.\n","date":"6 November 2023","externalUrl":null,"permalink":"/posts/datascience-stack/","section":"Posts","summary":"A collection of libaries and tools that I use to analyze data with Python","title":"My data science tools in 2023","type":"posts"},{"content":"","date":"1 May 2023","externalUrl":null,"permalink":"/tags/arduino/","section":"Tags","summary":"","title":"Arduino","type":"tags"},{"content":"","date":"1 May 2023","externalUrl":null,"permalink":"/tags/hardware/","section":"Tags","summary":"","title":"Hardware","type":"tags"},{"content":"","date":"1 May 2023","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"If you\u0026rsquo;ve worked with Arduinos before, you know that beeing constrained to the Arduino IDE can be quite limiting. PlatformIO is a cross-platform, cross-architecture, and multiple framework tool that can help with this. It is designed to make hardware development more comfortable and more efficient.\nPlatformIO is typically advertised as a VSCode plugin, which turns VSCode into an Arduino IDE-like platform. However, there is a lesser-known feature that developers can take advantage of - the simple command-line toolbox. In this blog post, we will walk you through how to install and use PlatformIO via the command line.\nTo begin, PlatformIO is a Python package, so it can be installed from PyPI, including within a virtual environment. However, it is highly recommended to use PlatformIO\u0026rsquo;s installation script instead. Here\u0026rsquo;s how you can install PlatformIO using the installation script:\ncurl -fsSL https://raw.githubusercontent.com/platformio/platformio-core-installer/master/get-platformio.py -o get-platformio.py python3 get-platformio.py During installation, PlatformIO creates its virtual environment, which is beneficial since it doesn\u0026rsquo;t interfere with system packages. However, the standard shell doesn\u0026rsquo;t recognize the PlatformIO executables, so we need to create symbolic links. You can do that using the following commands:\nln -s ~/.platformio/penv/bin/platformio ~/.local/bin/platformio ln -s ~/.platformio/penv/bin/pio ~/.local/bin/pio ln -s ~/.platformio/penv/bin/piodebuggdb ~/.local/bin/piodebuggdb On a Linux system, you also need to add udev rules. This snippet downloads the rules, places them in the appropriate location, reloads the udev service, and adds the current user to the relevant groups:\ncurl -fsSL https://raw.githubusercontent.com/platformio/platformio-core/develop/platformio/assets/system/99-platformio-udev.rules | sudo tee /etc/udev/rules.d/99-platformio-udev.rules To restart udev on debian-based distributions, run:\nsudo service udev restart sudo usermod -a -G dialout $USER sudo usermod -a -G plugdev $USER On arch-based distributions, use the following instead:\nsudo udevadm control --reload-rules sudo udevadm trigger sudo usermod -a -G uucp $USER sudo usermod -a -G lock $USER Log out and log back in for the changes to take effect.\nBy default, PlatformIO enables telemetry. However, you can disable it by running:\npio settings set enable_telemetry No pio settings get Next, you can initialize pio in your directory by creating an example project directory, for example, mkdir pio_test \u0026amp;\u0026amp; cd pio_test. Then, list all the available boards (platforms) by running pio boards. Choose your board and run:\npio project init --ide vim --board teensy41 Now you can edit the source code using your editor of choice. To test if the code compiles, you can simply run:\npio run To flash an attached board, add the following:\npio run --target upload For convenience, I personally created shell aliases comp for compilation and flash to upload. If you let your device access the serial port to print some debug information, you can monitor this by running:\npio device In conclusion, PlatformIO is a powerful tool for hardware development that provides a flexible and efficient workflow for software developers working with hardware. With its support for multiple platforms, frameworks, and libraries, along with a command-line interface and integrated debugging, PlatformIO is an excellent choice for anyone looking to streamline their hardware development workflow.\nPhoto by Vishnu Mohanan on Unsplash\n","date":"1 May 2023","externalUrl":null,"permalink":"/posts/platformio-introduction/","section":"Posts","summary":"An introduction to PlatformIO, a tool that makes software development for microcontrollers easier.","title":"PlatformIO: Hardware dev from the command line","type":"posts"},{"content":"With knowledge in applied machine learning, software development, and data analysis, I am currently pursuing master’s degrees in Neurobiology and Evolution and Ecology. My focus is on utilizing data science techniques to analyze the social behavior of electric fish.\nMy skills include converting unstructured data into structured formats for statistical analysis, particularly with audio and electric recordings or other sensor data. Proficient in Python, I use libraries like PyTorch, SciPy, and scikit-learn. I excel in data wrangling, building data pipelines, and managing multi-terrabyte datasets, with experience in supervised and unsupervised learning, dimensionality reduction, and fine-tuning machine learning models such as MLPs, CNNs, YOLO, and RCNN.\nI have presented my research at prominent conferences, demonstrating my ability to communicate complex data insights effectively. I have strong data visualization skills, using Matplotlib and Seaborn to create customized visualizations that meet high scientific standards.\nFor more information about my work and to discuss potential collaborations, please visit my projects page or view my resume. I look forward to connecting with you!\n","date":"25 February 2023","externalUrl":null,"permalink":"/about/","section":"Welcome","summary":"With knowledge in applied machine learning, software development, and data analysis, I am currently pursuing master’s degrees in Neurobiology and Evolution and Ecology.","title":"About","type":"page"},{"content":"Here is an overview of my recent personal and research projects.\n","date":"25 February 2023","externalUrl":null,"permalink":"/projects/","section":"Data Analysis Projects","summary":"Here is an overview of my recent personal and research projects.","title":"Data Analysis Projects","type":"projects"},{"content":"This is a small selection of projects and activities that I currently pursue in my free time (if there is any).\nLinux and open source software I am a huge fan of Linux and open source software. I got into Linux when I started working in the Neuroethology lab at the University of Tuebingen, where everybody was using it. The idea of a free and open operating system that is not only highly customizable but also very secure instantly fascinated me. Linux has been my main operating system for approximately 3 years now. I am currently running Arch Linux with the Hyprland window manager on both my personal laptop and work PC. I keep a dotfiles repository on GitHub where I store my configuration files and scripts.\nweygoldt/dots My personal dotfiles. Python 0 0 Scuba diving I am a certified PADI Advanced Open Water Diver. Diving always fascinated me but doing my Bachelors thesis in a marine biology lab provided the spark to finally get certified. At this time I have been diving in Germany, Indonesia and the Mediterranean Sea. I am really excited to continue my education as a diver, a skill that I\u0026rsquo;m sure will come in handy for my future research. I am particulary drawn towards the idea of becoming a technical- and cave diver to be able to explore challenging environments. In 2023, I got a first chance to combine this skill with my work during a four-week stay at the STARESO marine research station where I assisted PhD-students during sample collection dives.\nScorpion-fish catching in the Mediterranean sea. Photo courtesy of Lena Wesenberg To plan and log my dives, I use Subsurface, an open source dive log software written by no other than the creator of the Linux kernel, Linus Torvalds.\nHiking and everything outdoors Before I started my studies I spent my weekends with photography trips, mostly hiking in the mountains. Nowadays, I don\u0026rsquo;t have as much time for photography as I used to, but I still enjoy hiking, even if I rarely take my camera. The image below is one of my favorite pictures from that time, a self-portrait taken on a bivouac in the Alps.\nA self-portrait taken on a bivouac in the alps close to the Grimsel pass in Switzerland. For more images, check out my Instagram account or my Youtube channel for some videos of my trips.\n","date":"25 February 2023","externalUrl":null,"permalink":"/personal/","section":"What I do for fun","summary":"This is a small selection of projects and activities that I currently pursue in my free time (if there is any).","title":"What I do for fun","type":"personal"},{"content":"","date":"8 December 2022","externalUrl":null,"permalink":"/tags/caimg/","section":"Tags","summary":"","title":"Caimg","type":"tags"},{"content":"Experimental research has shown that zebrafish are less likely to perceive motion when the only moving element is color. Instead, it is the differences in brightness between moving stimuli that trigger a response (Orger and Baier, 2005). As part of a lab rotation project, we investigated how this behavior manifests in the optic tectum, the primary visual processing center in fish.\nTo achieve this, we employed two-photon calcium imaging to record the activity of direction selective neurons in the optic tectum of zebrafish larvae. Additionally, we simultaneously measured the optokinetic response, a behavioral indicator of motion perception.\nThe resulting dataset consisted of a z-stack of fluorescence microscopy images and the eye movements of the zebrafish embryo. After segmenting the cells with suite2p we analyzed the calcium activity (i.e., the luminance of each cell) and the eye velocities with respect to the stimulus that was shown to the fish.\nThe ensuing graphs illustrate a comparable pattern between the calcium activities (neural signal) and eye velocities (behavioral output) for the same stimulation conditions. Calcium acitivty in the Zebrafish optic tectum when stimulated with different levels of chromatic and achromatic contrasts. Behavioral output measured in the eye velocities during the optokinetic response shows a similar pattern compared to the neural activity. Our analysis suggested that the direction selective neurons in the optic tectum are likely colorblind, as they primarily respond to variations in brightness rather than color distinctions. As a result of this experiment, we have produced not only a poster but also a more comprehensive report, both of which can be found in the GitHub repository provided below.\nweygoldt/colorblind-directioncells A lab rotation with the systems neuroscience lab, University of Tuebingen, where we tested whether color contrast contributed to encoding motion in direction selective cells of the zebrafish optic tectum. Jupyter Notebook 0 0 Image by Zeiss\n","date":"8 December 2022","externalUrl":null,"permalink":"/projects/colorbling_direction_cells/","section":"Data Analysis Projects","summary":"Some neurons in thet zebrafish brain respond to movement direction. We recorded and analyzed the activity of these neurons and showed, that these cells probably rely on luminance alone to perceive motion.","title":"Colorblind direction cells","type":"projects"},{"content":"","date":"8 December 2022","externalUrl":null,"permalink":"/tags/neuroscience/","section":"Tags","summary":"","title":"Neuroscience","type":"tags"},{"content":"","date":"8 December 2022","externalUrl":null,"permalink":"/tags/zebrafish/","section":"Tags","summary":"","title":"Zebrafish","type":"tags"},{"content":"","date":"7 November 2022","externalUrl":null,"permalink":"/tags/covariance/","section":"Tags","summary":"","title":"Covariance","type":"tags"},{"content":"While sifting through a two-week continuous recording of electric fish in their natural habitat, I observed synchronous frequency modulations on a scale of seconds to two ten minutes between two fish. To detect these modulations, I developed a covariance-based detector, which I then used to detect these events on recordings. The following plot shows some examples of detected modulations: Synchronous rises of the EODf betweeen two individuals recorded in freely interacting fish in their natural habitat. To detect spatio-temporal interactions, particularly when fish chased each other, I build a cost function that peaks when (1) fish swim in the same trajectories relative to each other, (2) velocities of both fish increase, and (3) fish accelerate rapidly. By analyzing the estimated positions of fish over time, I demonstrated that those involved in frequency duets approach one another following the initiation of their \u0026ldquo;choir.\u0026rdquo; I also rendered videos depicting some of these interactions: Fish moving as data points on an electrode grid and their frequencies changing on the right-hand side.\nThe resulting output from this effort was displayed as a poster at the 2022 International Conference of Neuroethology (ICN). This poster can be accessed through the link provided in the GitHub repository below.\nweygoldt/synchronous-modulations Working directory including scripts for the analysis of synchronised EOD frequency modulations in freely interacting Brown Ghost Knifefish recorded in Colombia. Jupyter Notebook 0 0 ","date":"7 November 2022","externalUrl":null,"permalink":"/projects/electric_duet/","section":"Data Analysis Projects","summary":"Some electric fish synchronously modulate their frequencies in duets. I detected these events using a custom covariance-based event detector on frequency estimates from spectrograms.","title":"Electric duet","type":"projects"},{"content":"I use this mostly to share technical documentations to things that I think might be useful to others as well. If you have suggestions or comments, feel free to email me!\n","date":"13 June 2022","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"I use this mostly to share technical documentations to things that I think might be useful to others as well.","title":"Posts","type":"posts"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]