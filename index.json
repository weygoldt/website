[{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/bayesian/","section":"Tags","summary":"","title":"Bayesian","type":"tags"},{"content":" When I first explored Bayesian methods, one of the main challenges was understanding the concept of \u0026ldquo;sampling.\u0026rdquo; What does it mean to sample from the posterior? How does it differ from frequentist fitting approaches, and how is it implemented in code? A straightforward example to explore these questions is a simple implementation of the Metropolis-Hastings (MH) algorithm. This Markov Chain Monte Carlo (MCMC) method is used to sample from the posterior distribution of a Bayesian model. Although it is less sophisticated than modern approaches like the No-U-Turn Sampler used in PyMC and Stan, it remains a valuable tool for building intuition about what happens behind the scenes.\nLet\u0026rsquo;s first define the problem we want to solve:\nConsider a very simple Bayesian model: you have recorded the choices of many subjects in a binary decision task, where each subject can choose either 0 or 1. We are interested in estimating the population bias towards a particular choice. While you could simply compute the relative frequencies of zeros and ones to estimate this bias, our goal is to also quantify the uncertainty associated with that estimate. This is where our binary choice model comes into play.\nSince we recorded count data, we can model the likelihood function using a simple Bernoulli model:\n$$ y_i \\sim \\text{Bernoulli}(\\theta) $$\nwhere \\(y\\) represents the choices of the subjects and \\(\\theta\\) is the population bias.\nBecause we have no strong prior expectations about the bias, we will use a weakly informative prior. We know that \\(\\theta\\) is a probability (hence it lies between 0 and 1), so we choose a Beta distribution that is uniform between 0 and 1 for our prior on \\(\\theta\\):\n$$ \\theta \\sim \\text{Beta}(1, 1) $$\nTo sample from the posterior, let us first import the necessary packages and define some helper functions:\nimport numpy as np from scipy.stats import beta, bernoulli, norm, uniform import seaborn as sns import matplotlib.pyplot as plt from rich.progress import track import pandas as pd np.random.seed(42) def compute_hdi(samples, cred_mass=0.95): \u0026#34;\u0026#34;\u0026#34; Compute the Highest Density Interval (HDI) for a given array of samples. \u0026#34;\u0026#34;\u0026#34; n_samples = len(samples) sorted_samples = np.sort(samples) # Number of samples required to cover the desired credible mass interval_idx_inc = int(np.floor(cred_mass * n_samples)) # e.g., 4750 # Total number of possible intervals in the sorted samples n_intervals = n_samples - interval_idx_inc # Slide a window across the sorted samples interval_starts = sorted_samples[:n_intervals] interval_ends = sorted_samples[interval_idx_inc:] interval_widths = interval_ends - interval_starts # Find the index of the smallest interval min_idx = np.argmin(interval_widths) # The HDI is the smallest interval hdi_min = sorted_samples[min_idx] hdi_max = sorted_samples[min_idx + interval_idx_inc] return hdi_min, hdi_max Now, let\u0026rsquo;s define the likelihood function. As discussed, we use the Bernoulli distribution from scipy.stats. Our function returns the probability of the data given the current value of $\\theta$. In practice, we build a Bernoulli distribution with the current \\(\\theta\\) using bernoulli(theta), then evaluate the probability mass function (.pmf) for each data point. Assuming independent observations, we multiply these probabilities together to obtain the likelihood of the full dataset.\ndef bernoulli_likelihood(theta, data): return bernoulli(theta).pmf(data).prod() Next, we need a function that computes the unnormalized posterior probability—that is, the product of the prior and the likelihood, corresponding to the numerator of Bayes\u0026rsquo; Theorem.\nOur function will take the current \\(\\theta\\), the data, the prior distribution (our Beta distribution), and the likelihood function (defined above). First, we sanity-check whether \\(\\theta\\) is between 0 and 1. If it is, we compute the prior probability of \\(\\theta\\), evaluate the likelihood of the data given \\(\\theta\\), and then compute the unnormalized posterior by multiplying these two values together. Note that this is Bayes\u0026rsquo; Theorem in action, but without the normalizing constant.\ndef posterior_probability(theta, data, prior_dist, likelihood_func): if 0 \u0026lt;= theta \u0026lt;= 1: prior_prob = prior_dist.pdf(theta) # Prior probability of theta likelihood = likelihood_func(theta, data) # Likelihood of data given theta posterior_prob = ( prior_prob * likelihood ) # Unnormalized posterior (Bayes\u0026#39; Theorem without the constant) return posterior_prob else: return -np.inf Now that we have defined the two necessary functions, let\u0026rsquo;s initialize the sampler and sample from the posterior. First, we generate some data and plot it to visualize what it looks like:\ndata = bernoulli(0.7).rvs(20) vals, counts = np.unique(data, return_counts=True) vals_prob = counts / len(data) print(f\u0026#34;Outcomes: {vals}, Frequencies: {vals_prob}\u0026#34;) fig, ax = plt.subplots(constrained_layout=True) plot_data = pd.DataFrame({\u0026#34;choice\u0026#34;: vals, \u0026#34;count\u0026#34;: counts}) sns.barplot(plot_data, x=\u0026#34;choice\u0026#34;, y=\u0026#34;count\u0026#34;, palette=\u0026#34;deep\u0026#34;, hue=\u0026#34;choice\u0026#34;, ax=ax) ax.set_xlabel(\u0026#34;Binary Choice Outcome\u0026#34;) ax.set_ylabel(\u0026#34;Count\u0026#34;) plt.savefig(\u0026#34;hist.svg\u0026#34;) plt.show() Now we can define the prior distribution and our likelihood function.\nprior_dist = beta(a=1, b=1) # theta ~ Beta(1, 1) likelihood = bernoulli_likelihood # y ~ Bernoulli(theta) sns.histplot(prior_dist.rvs(10000), kde=True, stat=\u0026#34;density\u0026#34;) plt.savefig(\u0026#34;prior.svg\u0026#34;) plt.show() Next, let\u0026rsquo;s initialize the sampler. We set the sampler to run for 5000 iterations and create an array to store the samples. For the initial value of \\(\\theta\\), we choose the mean of the prior distribution (0.5) as a starting point. We also define a proposal distribution from which new candidate values of \\(\\theta\\) are drawn. Here, we use a simple normal distribution with a standard deviation of 0.05.\nnum_iterations = 5000 samples = np.zeros(num_iterations) current_theta = prior_dist.mean() # Initialize theta at the mean of the prior (0.5) proposal_std = 0.05 # Standard deviation of the proposal distribution We can now estimate the unnormalized posterior probability of the current initial value of $\\theta$:\ncurrent_posterior = posterior_probability(current_theta, data, prior_dist, likelihood) With everything set up, let\u0026rsquo;s run the sampler. In each iteration, the sampler first draws a new candidate from the proposal distribution and computes its unnormalized posterior probability (i.e., the probability of the candidate given the data).\nUsing the current and candidate unnormalized posteriors, we compute the acceptance ratio by simply dividing the candidate\u0026rsquo;s posterior by the current posterior. This is the key step in the MH algorithm—notice how the marginal probability (which would be difficult to compute) cancels out in the ratio.\nThis step is what gives the algorithm its Markov Chain property: the decision to move to the candidate state depends solely on the current state and the acceptance ratio. By generating a random number between 0 and 1 and comparing it to the acceptance ratio, we decide whether to accept the candidate. This stochastic acceptance is crucial for ensuring that the chain can escape local maxima and thoroughly explore the tails of the distribution and is also the Monte-Carlo sampling part of the algorithm.\nfor i in track(range(num_iterations), description=\u0026#34;Sampling posterior...\u0026#34;): # Propose a new candidate (Monte Carlo Sampling) candidate_theta = norm(current_theta, proposal_std).rvs() # Compute the posterior probability of the candidate candidate_posterior = posterior_probability( candidate_theta, data, prior_dist, likelihood, ) # Compute the acceptance ratio acceptance_ratio = candidate_posterior / current_posterior # (Markov Chain update) # Both are computed as unnormalized posteriors: prior * likelihood # The normalizing constant cancels out! # Accept the candidate with probability equal to the acceptance ratio if acceptance_ratio \u0026gt; uniform(0, 1).rvs(): current_theta = candidate_theta current_posterior = candidate_posterior samples[i] = current_theta After drawing all the samples, we can examine the trace of our samples across iterations and view the resulting posterior distribution (which is simply the distribution of our samples). For example, we can compute the mean of the posterior, which turns out to be around 0.68, with a 95% HDI of [0.49, 0.85]. Thus, we can say:\nGiven our prior, the data, and our model, there is a 95% chance that the true bias lies between 0.49 and 0.85.\nNote that this wide interval is partly due to the fact that we simulated only 20 data points.\nmean_theta = samples.mean() hdi_min, hdi_max = compute_hdi(samples, cred_mass=0.95) print(f\u0026#34;Mean theta = {mean_theta:.2f}, 95% HDI = [{hdi_min:.2f}, {hdi_max:.2f}]\u0026#34;) Examining the trace and the posterior distribution, we see that the posterior is centered around 0.7 with a 95% HDI of [0.49, 0.85]. The trace plot shows that while we started at 0.5 (the mean of our uniform Beta prior), the chain eventually moves towards around 0.7 and then fluctuates within a certain range. This range reflects the stationary distribution of our Markov Chain, which represents our posterior distribution.\nfig, axs = plt.subplots( 1, 2, width_ratios=[5, 1.5], constrained_layout=True, sharey=True ) axs[0].plot(samples, lw=1) sns.histplot(y=samples, ax=axs[1], kde=True, stat=\u0026#34;density\u0026#34;) axs[1].axhline(y=0.7, color=\u0026#34;grey\u0026#34;, label=\u0026#34;True theta = 0.7\u0026#34;) axs[1].axhline( y=mean_theta, color=\u0026#34;tab:red\u0026#34;, label=f\u0026#34;Mean theta = {mean_theta:.2f}\u0026#34;, ) axs[1].plot( [-0.3, -0.3], [hdi_min, hdi_max], color=\u0026#34;k\u0026#34;, label=f\u0026#34;95% HDI = [{hdi_min:.2f}, {hdi_max:.2f}]\u0026#34;, lw=3, ) axs[1].legend(bbox_to_anchor=(1.04, 1), borderaxespad=0) axs[0].set_title(\u0026#34;Trace\u0026#34;) axs[0].set_xlabel(\u0026#34;Iteration\u0026#34;) axs[0].set_ylabel(\u0026#34;Theta\u0026#34;) axs[1].set_title(\u0026#34;Posterior Density of Theta\u0026#34;) axs[1].set_xlabel(\u0026#34;Density\u0026#34;) axs[1].set_ylabel(\u0026#34;Theta\u0026#34;) plt.savefig(\u0026#34;trace.svg\u0026#34;) plt.show() Congratulations on working through the MH-algorithm! You now know how to sample from a posterior distribution using the Metropolis-Hastings algorithm. I invite you to run this script on your own machine and experiment with different parameters. For instance, you will quickly observe that generating more data causes the credible intervals around our parameter estimate for $\\theta$ to shrink. Additionally, try modifying the prior: if you skew the Beta distribution in one direction, you will significantly alter the shape of the posterior. This link will take you to the script I used to generate this post, you can just copy-paste it and run it.\nIn practice, these algorithms are extended to efficiently handle much more complex models. However, the MH-algorithm remains a great starting point for developing intuition about how modern probabilistic programming frameworks work behind the scenes.\nThanks for reading!\n","date":"26 February 2025","externalUrl":null,"permalink":"/posts/mh_algorithm/","section":"","summary":"Get an intuitive understanding of Bayesian inference by sampling the posterior \u0026lsquo;by hand\u0026rsquo;.","title":"Bayesian Inference from Scratch with the MH-Algorithm in Python","type":"posts"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/code/","section":"Tags","summary":"","title":"Code","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data-Science","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/documentation/","section":"Tags","summary":"","title":"Documentation","type":"tags"},{"content":"We’ve all been there: you find an exciting coding experiment, quickly copy the code, try to execute it, and it fails. Not because of missing dependencies, but due to missing functions, variables, or an improper code order. I hate that.\nAs someone who values simplicity and efficiency, I\u0026rsquo;ve often struggled with tools like Jupyter Notebooks. They offer robust features but can complicate version control, file management, and often require heavyweight IDEs. As a Neovim enthusiast, I prefer plain text solutions. This led me to a novel approach:\nWhat if you could write your entire article in a simple Python file—using comment blocks for text and converting it to Markdown?\nThis method simplifies writing, keeps everything in a single file, and integrates perfectly with version control. Plus, it ensures that the file is executable and easy to run, making it a self-contained and efficient solution.\nHere\u0026rsquo;s the plan:\nUse top-level block comments (enclosed by triple quotes) for text. Treat the rest as code. A program to achieve this needs to:\nExtract text from top-level block comments. Wrap Python code in Markdown code blocks. Let\u0026rsquo;s explore how to do this using standard Python libraries.\nfrom dataclasses import dataclass from pathlib import Path import argparse import re from typing import List import typer app = typer.Typer() We structure the file into blocks. Each block is either a text (Markdown) block or a code block.\n@dataclass class Block: content: str is_code: bool Check if the given line marks the start or end of a block comment.\ndef is_comment(line: str) -\u0026gt; bool: return line.startswith(\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;) or line.startswith(\u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;) Remove triple-quote syntax from a line. If the line consists solely of triple quotes (with possible whitespace), return an empty string.\ndef remove_comment_syntax(line: str) -\u0026gt; str: if line.strip() in (\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;, \u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;): return \u0026#34;\u0026#34; for token in [\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;, \u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;]: if line.lstrip().startswith(token): line = line.lstrip()[len(token) :] break for token in [\u0026#39;\u0026#34;\u0026#34;\u0026#34;\u0026#39;, \u0026#34;\u0026#39;\u0026#39;\u0026#39;\u0026#34;]: if line.rstrip().endswith(token): line = line.rstrip()[: -len(token)] break return line Unescape mathjax notation in a text by replacing double backslashes with a single backslash. This is applied only to non-code (Markdown) text.\ndef unescape_mathjax(text: str) -\u0026gt; str: return text.replace(\u0026#34;\\\\\\\\\u0026#34;, \u0026#34;\\\\\u0026#34;) Extract blocks of code and text from the given file. The control flow has been inverted so that non-comment lines are handled immediately.\ndef extract_blocks(file_path: Path) -\u0026gt; List[Block]: with file_path.open(\u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: lines = f.readlines() blocks: List[Block] = [] in_comment_block = False comment_lines: List[str] = [] code_lines: List[str] = [] for line in lines: # Non-comment markers: process immediately. if not is_comment(line): if in_comment_block: comment_lines.append(line) else: code_lines.append(line) continue # Here, the line is a comment marker. if in_comment_block: comment_lines.append(remove_comment_syntax(line)) blocks.append(Block(\u0026#34;\u0026#34;.join(comment_lines), is_code=False)) comment_lines.clear() in_comment_block = False continue # Starting a new comment block. if code_lines: blocks.append(Block(\u0026#34;\u0026#34;.join(code_lines), is_code=True)) code_lines.clear() comment_lines.append(remove_comment_syntax(line)) in_comment_block = True if code_lines: blocks.append(Block(\u0026#34;\u0026#34;.join(code_lines), is_code=True)) return blocks Build a Markdown file from the blocks. Wrap code blocks in Markdown code fences and process mathjax unescaping in text blocks.\ndef build_markdown(blocks: List[Block], output_path: Path) -\u0026gt; None: output: List[str] = [] for block in blocks: if not block.content.strip(): continue if block.is_code: output.append(\u0026#34;\\n```python\\n\u0026#34; + block.content + \u0026#34;\\n```\\n\u0026#34;) else: # Unescape mathjax by removing redundant backslashes. output.append(unescape_mathjax(block.content)) markdown_text = \u0026#34;\u0026#34;.join(output) markdown_text = re.sub(r\u0026#34;\\n{3,}\u0026#34;, \u0026#34;\\n\\n\u0026#34;, markdown_text) with output_path.open(\u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(markdown_text) Tie everything together: parse arguments, extract blocks, and build Markdown output.\n@app.command() def main(python_file: Path, markdown_file: Path) -\u0026gt; None: blocks = extract_blocks(python_file) build_markdown(blocks, markdown_file) if __name__ == \u0026#34;__main__\u0026#34;: app() You can now run this script on any Python file from the terminal. For example, if saved as py2md.py, run:\npython3 py2md.py py2md.py out.md Alternatively, install it directly from GitHub:\npip install git+https://github.com/weygoldt/py2md Then, within the repository, simply run:\npy2md py2md/main.py README.md This command will generate the article you’re reading now as a Markdown file. Happy writing!\n","date":"26 February 2025","externalUrl":null,"permalink":"/posts/py2md/","section":"","summary":"Turn your Python code into Markdown with ease","title":"How I Write Articles Using Plain Python Files","type":"posts"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine-Learning","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"Markdown","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/neovim/","section":"Tags","summary":"","title":"Neovim","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/categories/post/","section":"Categories","summary":"","title":"Post","type":"categories"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/probability/","section":"Tags","summary":"","title":"Probability","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/statistics/","section":"Tags","summary":"","title":"Statistics","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/tools/","section":"Tags","summary":"","title":"Tools","type":"tags"},{"content":" PhD student at the Neuroethology Group @UniTue.\nInterested in Data Science \u0026amp; Quantitative Behavior of electric fish.\nLearn more! ","date":"26 February 2025","externalUrl":null,"permalink":"/","section":"Welcome","summary":"","title":"Welcome","type":"page"},{"content":"","date":"26 February 2025","externalUrl":null,"permalink":"/tags/writing/","section":"Tags","summary":"","title":"Writing","type":"tags"},{"content":"One of my favourite activities when working with data is to visualize it. I enjoy creating plots, diagrams, and rendering videos to communicate complex information in a simple and appealing way.\nStandalone plots # Here you see some of my data visualizations exclusively rendered in matplotlib without additional edits from graphical editors. Most of them are from my Masters Thesis.\nConference posters # The following list includes some posters from University projects as well as conference presentations. All posters are exclusively rendered in LaTex.\nTitle Presented Link Complex frequency modulations in freely interacting electric fish recorded in their natural habitat International Conference of Neuroethology (ICN), Lisbon 2022 pdfgithub Automated detection of chirps in electric fish: Deep learning applied to ethology Annual Meeting of the Ethological Society, Muenster 2024 \u0026 International Conference of Neuroethology (ICN), Berlin 2024 pdf Data videos # These were created by producing series of individual frames with Matplotlib and combining them into videos with ffmpeg.\n","date":"20 September 2024","externalUrl":null,"permalink":"/posts/dataviz/","section":"","summary":"A gallery of recent data visualizations, from simple plots to conference posters and even a video.","title":"A collection of data visualizations","type":"posts"},{"content":"Hi, I\u0026rsquo;m Patrick! I am a behavioral biologist with a passion for data analysis and fieldwork. After finishing two Masters degrees, one in Neurobiology and one in Evolution and Ecology, I am now pursuing a PhD at the Neuroethology Lab at the University of Tuebingen. My research focuses on the social behavior of electric fish, where I apply advanced data analysis techniques to unstructured data from passive electric recorders in the field to learn about the social behavior of electric fish.\nWhat drives my research # Why are some species social while others are not? How do animals communicate with each other, and what information do they convey? These questions drive my research, which lies at the intersection of ecology, behavior, and neurobiology. I am broadly interested in how these fields can be integrated to understand the development and function of social behaviors and communication in animals.\nElectric fish offer a unique opportunity to study these phenomena as they offer a wide variety of social lives that can be explored through electric signals. We design and deploy electrode arrays and required analysis software to monitor electric fish in their natural habitats. This non-invasive technology captures electrolocation- and communication signals and tracks movement, providing detailed, quantitative observation of behavior in the wild—an approach that is unique to electric fish.\nElectrode grid recording the electric field of a fish. Modified after Henninger et al. 2018 Data analysis # I mainly work with Python and use libraries like PyTorch, SciPy, and scikit-learn for data analysis. I like data wrangling, building data pipelines, and managing multi-terrabyte datasets, and have experience in supervised and unsupervised learning, dimensionality reduction, and fine-tuning deep learning models such as MLPs, CNNs, YOLO, and RCNN. I really enjoy creating visualizations, some of which you can find on here:\nA collection of data visualizations 20 September 2024 A gallery of recent data visualizations, from simple plots to conference posters and even a video. Fieldwork # Aside from data analysis, I really enjoy the outdoors. I am a certified Advanced Open Water Diver and have been diving in Germany, Indonesia and the Mediterranean Sea. I have fieldwork experience in the Amazon rainforest studying electric fish, and have worked at the STARESO marine research station in Corsica, where I assisted PhD-students during sample collection dives. I\u0026rsquo;m somebody who enjoys the challenge of working in remote locations and difficult conditions, and I am always looking for new opportunities to expand my fieldwork experience. Aside from the rainforest and underwater, I would especially love to do some work in caves at some point in the future.\nPrevious Nextsads A report on my latest trip to the Amazon rainforest can be found here:\nExploring Electric Fish in the Rainforest of Brazil 7 November 2023 A small report on a field trip to the Amazonian Rainforest in Amapa, Brazil in 2023 Open science and inclusivity # As a PhD student, I’m passionate about quantitative, reproducible, and open science. I believe in sharing knowledge through open-access publications, openly available code, and open hardware designs, making research more collaborative and accessible. You can find anything including analysis code, poster and presentation templates and my Linux system configuration on my GitHub page. I also value fostering a welcoming and respectful environment where everyone feels included and supported. I strive to approach my work and collaborations with empathy and an openness to diverse perspectives to encourage meaningful contributions from everyone involved.\nGet in touch # I am always open to new collaborations and opportunities. If you are interested in my work or have any questions, feel free to reach out to me via email.\n","date":"20 September 2024","externalUrl":null,"permalink":"/about/","section":"Welcome","summary":"","title":"About me","type":"page"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/datascience/","section":"Tags","summary":"","title":"Datascience","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/dataviz/","section":"Tags","summary":"","title":"Dataviz","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/design/","section":"Tags","summary":"","title":"Design","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/matplotlib/","section":"Tags","summary":"","title":"Matplotlib","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/categories/projects/","section":"Categories","summary":"","title":"Projects","type":"categories"},{"content":"In my research I apply advanced data analysis techniques to unstructured data from passive electric recorders in the field to learn about the social behavior of electric fish.\nCommunication and social behavior of electric fish in the wild # Why are some species social while others are not? How do animals communicate with each other, and what information do they convey? These questions drive my research, which lies at the intersection of ecology, behavior, and neurobiology. I am broadly interested in how these fields can be integrated to understand the development and function of social behaviors and communication in animals.\nElectric fish offer a unique opportunity to study these phenomena as they offer a wide variety of social lives that can be explored through electric signals. We design and deploy electrode arrays and required analysis software to monitor electric fish in their natural habitats. This non-invasive technology captures electrolocation- and communication signals and tracks movement, providing detailed, quantitative observation of behavior in the wild—an approach that is unique to electric fish.\nUsing data analyses and machine learning, we process the large-scale datasets generated from field recordings, which allows us to detect subtle patterns in communication and movement. I aim to observe, compare, and understand social interactions in electric eels and other species of electric fish in their natural environments.\nBy combining the study of communication signals and movement patterns, I seek to uncover how and why animals interact during courtship, competition, and foraging. This way, we might start to understand how communication mediates complex social behaviors in species where little is currently known, let alone observable in the wild.\nElectric fish have already been a focus during my master\u0026rsquo;s studies, which resulted in the following two projects:\nChirp detector 18 November 2023 Electric fish produce fast frequency sweeps to communicate, called chirps. Here, I detected these chirps with a CNN-classifier on spectrograms that was trained on data I simulated. Electric duet 7 November 2022 Some electric fish synchronously modulate their frequencies in duets. I detected these events using a custom covariance-based event detector on frequency estimates from spectrograms. ","date":"20 September 2024","externalUrl":null,"permalink":"/projects/","section":"Welcome","summary":"","title":"Research projects","type":"page"},{"content":"I really enjoy writing code, whether it is for data analysis, visualization or to automate small tasks in my daily life. Here, you\u0026rsquo;ll find a section with some of these projects. Most of them are work in progress and I usually don\u0026rsquo;t have the time to polish them to a point where I would consider them \u0026ldquo;finished\u0026rdquo;. I just continue to develop features as I need them.\nchirpdetector # is a Python package that leverages YOLO models to detect electrocommunication signals on spectrograms of terrabytes of recordings of electric fish. The model backend is implemented as a \u0026ldquo;plug and play\u0026rdquo; system, allowing for easy integration of new models. The main selling point of this package is the framework to efficiently parse and process large amounts of data.\nA previous prototype of this package using a simple CNN classifier was described in this article:\nChirp detector 18 November 2023 Electric fish produce fast frequency sweeps to communicate, called chirps. Here, I detected these chirps with a CNN-classifier on spectrograms that was trained on data I simulated. The current version of the package is still in development, but you can check out the repository for more information:\nweygoldt/chirpdetector Detect brief communication signals of weakly electric fish in the frequency domain Python 3 1 gridtools # is a Python package that provides a data model to load, process and save multi-channel electrode grid data including metadata and derived features, such as the communication signals and position estimates of the fish. The data model is designed to be easily extendable and implemented using pydantic to ensure type safety. I am also working on extending simulation features to generate synthetic data for testing and training purposes.\nweygoldt/gridtools Overhaul electrode grid preprocessing routine Python 0 0 reportable # is a Python package that provides a simple way to \u0026ldquo;extract\u0026rdquo; reports from larger data science repositories. Sometimes, I find myself in a situation where I have a report, such as a slide deck, that links to a few plots in a directory with many other plots. This package helps me to extract only the relevant media files and copy them to a new directory and update the links in the report. This makes it easier to share the report with others without sharing the entire repository.\nweygoldt/reportable Makes scripted reports (LaTeX, Markdown, Quarto) portable. Python 0 0 py2md # is a Python package that converts Python files to Markdown files. Let me explain: I hate Jupyter Notebooks. I really prefer to just have plain text files to make version control, and file management easier. So I came up with the idea to write my articles in Python files using block comments for text and the rest as code. This package extracts the text from the comments and wraps the code in Markdown code blocks. This way, I can write my articles in a single file and convert them to Markdown. An additional benefit is that the file is executable and easy to run, making it a self-contained and efficient solution. This article explains the process in more detail and at the same time, is the actual output of the package after running it on itself:\nHow I Write Articles Using Plain Python Files 26 February 2025 Turn your Python code into Markdown with ease Check out the repository for more information:\nweygoldt/py2md Convert python files with markdown formatting in top-level block comments to nicely formatted markdown files with python code blocks 📝 Python 0 0 slicepy # creates streamlined lists of important code edits. During rapid prototyping phases, I find myself writing tons of TODO comments which I then forget about. This package extracts these comments along with some context from a code base and then queries the OpenAI-API to generate a summary of every TODO item. The result is a markdown file with a checklist of all TODO items and their context, placed in the root directory of the project.\nweygoldt/slicepy Streamlined lists of important code edits - A tool to manage TODO comments in codebases. Python 0 0 audioviz # is a Python package that provides a simple way to visualize audio data. Running it on an audio file, it creates an animated video of the spectrogram and the audio waveform in sync with the audio track. This package is still in the early stages of development, but I plan to extend it to include more features as soon as I need it for presentations or blog posts.\nweygoldt/audioviz Turn audio into animated spectrogram videos for presentations Python 0 0 dotfiles # are a collection of configuration files for my Linux system. I use them to set up new systems quickly and to keep my system configuration consistent across different machines. This way, I can easily switch between my work laptop and my home desktop without having to worry about different configurations. They are highly opinionated and tailored to my needs, but feel free to use them as a starting point for your own configuration.\nweygoldt/dots My personal dotfiles. Python 0 0 ","date":"20 September 2024","externalUrl":null,"permalink":"/software/","section":"Welcome","summary":"","title":"Software projects","type":"page"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/creativecoding/","section":"Tags","summary":"","title":"Creativecoding","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/datavis/","section":"Tags","summary":"","title":"Datavis","type":"tags"},{"content":"Random walks are a beautifully simple concept that many beginners encounter in their coding journey. These basic algorithms can be extended to create stunning visualizations. Below, you\u0026rsquo;ll see how enhancing a simple random walk can result in a captivating display:\nA standard random walk typically restricts movement to four cardinal directions. However, by allowing the walk to step in any angular direction, we can create far more dynamic patterns. I first explored this idea during my master\u0026rsquo;s thesis on simulating the movement of electric fish, and later found inspiration in a Reddit post that employed a similar concept.\nThe real beauty of this approach lies not just in the freedom provided by the unit circle but also in how we can purposefully limit this freedom. If an agent were to step in a completely random direction at each juncture, the path would appear highly chaotic. Typically, \u0026ldquo;real\u0026rdquo; agents (whatever they might represent) move in relatively straight trajectories most of the time.\nTo achieve more natural movement, rather than selecting a direction at random each time, we can utilize a probability density function (PDF) to influence the likelihood of choosing certain directions based on the previous step\u0026rsquo;s direction. In this example, we\u0026rsquo;ll use a Gaussian PDF, although for simulating the motions of a Knifefish, I employed a bimodal Gaussian or a von Mises distribution to mimic the characteristic forward and backward movement of these fish.\nSuch configurations ensure that the trajectory of the subsequent step remains close to the previous step, with variations governed by the standard deviation of the Gaussian.\nLet\u0026rsquo;s jump into the code. First, we need to import some libraries and set up Matplotlib to use a dark theme.\nimport numpy as np import matplotlib.pyplot as plt import numpy as np import seaborn as sns from matplotlib.collections import LineCollection plt.rcParams.update( { \u0026#34;figure.facecolor\u0026#34;: \u0026#34;black\u0026#34;, \u0026#34;axes.facecolor\u0026#34;: \u0026#34;black\u0026#34;, \u0026#34;axes.edgecolor\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;axes.labelcolor\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;xtick.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;ytick.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;grid.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;text.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;legend.facecolor\u0026#34;: \u0026#34;black\u0026#34;, \u0026#34;legend.edgecolor\u0026#34;: \u0026#34;white\u0026#34;, } ) Now lets declare some parameters. Playing with them will substantially impact the output of our random walk.\nn_walkers = 500 n_steps = 500 starting_point = (0, 0) The initial_seed will be the possible trajectories that the first step of each walker is drawn from.\ninitial_seeds = np.linspace(0, 2 * np.pi, n_walkers) The gaussian_pdf_sigmas will be the standard deviations of the probability density functions that determine the the variability of the trajectory in the next step, given the current step.\ngaussian_pdf_sigmas = np.linspace(np.pi / 150, np.pi / 100, n_walkers) Let\u0026rsquo;s also select a colormap and introduce a circle, on which the random walkers start from.\ncmap = sns.color_palette(\u0026#34;mako\u0026#34;, as_cmap=True) circle_radius = 220 # Adjustable radius for the circle Now let\u0026rsquo;s randomly draw the initial trajectories for each walker as well as the standard deviation that determines the variability of each walker.\nseeds = np.random.choice(initial_seeds, n_walkers) pdf_sigmas = np.random.choice(gaussian_pdf_sigmas) The following lines set the initial position of each random walker onto the boundary of a circle that we can control with the circle_radius parameter.\n# Compute starting positions on the rim of the circle start_x = circle_radius * np.cos(seeds) start_y = circle_radius * np.sin(seeds) # Initialize positions array to store positions at each step for each walker positions = np.zeros((n_walkers, n_steps, 2)) positions[:, 0, 0] = start_x positions[:, 0, 1] = start_y The main loop for the random walk is straightforward but compelling in its simplicity. We are adjusting the direction slightly using a Gaussian PDF while updating the walker\u0026rsquo;s position on each step. This could probably be optimized to run faster, but at this point this is not nessecary and I think this is much more readable.\n# Perform the random walk for step in range(1, n_steps): # Generate directions based on Gaussian distribution seeds = np.random.normal(seeds, pdf_sigmas) # Calculate step increments dx = np.cos(seeds) dy = np.sin(seeds) # Update positions positions[:, step, 0] = positions[:, step - 1, 0] + dx positions[:, step, 1] = positions[:, step - 1, 1] + dy For the visualization, we differentiate the paths by their distance from the origin, adding an aesthetic dimension to the display.\n# Plot the paths of the walkers fig, ax = plt.subplots(figsize=(20, 20)) for i in range(n_walkers): # get x and y positions x = positions[i, :, 0] y = positions[i, :, 1] # compute distance to origin at (0,0) dist = np.sqrt(x**2 + y**2) points = np.array([x, y]).T.reshape(-1, 1, 2) segments = np.concatenate([points[:-1], points[1:]], axis=1) # Create a continuous norm to map from data points to colors norm = plt.Normalize(dist.min(), dist.max()) lc = LineCollection(segments, cmap=cmap, norm=norm) # Set the values used for colormapping lc.set_array(dist) lc.set_linewidth(1.5) lc.set_alpha(1) line = ax.add_collection(lc) ax.axis(\u0026#34;equal\u0026#34;) ax.axis(\u0026#34;off\u0026#34;) # plt.savefig(\u0026#34;cover1.jpg\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;, pad_inches=0, dpi=300) plt.show() I hope you find these visual results as fascinating as I do. With further adaptation, such as integrating principles from the Boids algorithm — which emphasizes coherence, separation, and alignment — we could guide the walkers into forming dynamic flocks as they evolve. This concept is something that we may explore in a future post.\n","date":"19 May 2024","externalUrl":null,"permalink":"/posts/angular_randomwalk/","section":"","summary":"Let\u0026rsquo;s see how basic random walks can be extended a bit to create interesting visuals.","title":"Pretty pictures with angular random walks","type":"posts"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/randomwalk/","section":"Tags","summary":"","title":"Randomwalk","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/simulation/","section":"Tags","summary":"","title":"Simulation","type":"tags"},{"content":"Understanding the significance of specific communication cues necessitates our ability to detect them, particularly with transient frequency modulation signals like chirps produced by an electric organ discharge in electric fish. Previous research has mainly focused on immobilizing or artificially stimulating fish or physically separating them, conditions unfavorable for natural communication.\nTo address this challenge, I designed a convolutional neural network-based detector capable of detecting chirps in freely behaving fish. Despite initially training the model on simulated data, it surprisingly performed well on real-world recordings after some fine-tuning. Using this version, I successfully detected approximately 50,000 chirps, marking the largest dataset at that time.\nThe following image illustrates a short segment of a recording featuring two fish. Chirps are visible as frequency fluctuations from the baseline of one of the two fish on a spectrogram. The dots indicate where the detector identified a chirp.\nA spectrogram of a recording of two fish. The dots indicate where the detector detected a chirp. Performing preliminary analyses utilizing this detector, we discovered that chirps could potentially be utilized by the losing fish to indicate submission during competition for a shelter among two fish.\nweygoldt/cnn-chirpdetector A first try of detecting the transient communication signals of weakly electric fish on a spectrogram using a convolutional neural network. Python 1 0 ","date":"18 November 2023","externalUrl":null,"permalink":"/posts/chirp_detector/","section":"","summary":"Electric fish produce fast frequency sweeps to communicate, called chirps. Here, I detected these chirps with a CNN-classifier on spectrograms that was trained on data I simulated.","title":"Chirp detector","type":"posts"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/cnn/","section":"Tags","summary":"","title":"Cnn","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/dsp/","section":"Tags","summary":"","title":"Dsp","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/efish/","section":"Tags","summary":"","title":"Efish","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/neuralnetwork/","section":"Tags","summary":"","title":"Neuralnetwork","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/spectrogram/","section":"Tags","summary":"","title":"Spectrogram","type":"tags"},{"content":"","date":"18 November 2023","externalUrl":null,"permalink":"/tags/timeseries/","section":"Tags","summary":"","title":"Timeseries","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer-Vision","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/dataset/","section":"Tags","summary":"","title":"Dataset","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep-Learning","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/label-studio/","section":"Tags","summary":"","title":"Label-Studio","type":"tags"},{"content":"Note: This guide was written using label-studio version 1.8.2.post1.\nWith the modules included in pytorch, opencv or ultralytics, training computer vision models is easier than ever, the main constraint being the availability of labeled data. While there are many \u0026ldquo;smart\u0026rdquo; labeling solutions, most of them are either expensive or not open source. Label-studio is unique in that it free to use, open source and even can be made \u0026ldquo;smart\u0026rdquo; by adding your own pre-labeling network for automatic labeling.\nBut setting up label-studio locally to correct pre-annotated images of my YOLO dataset that I passed through a Faster R-CNN model trained on synthetic data took me about two days. So I decided to write this in case I forget how I did it. And maybe it helps someone else before they waste two days of their life.\n1. Convert the dataset to label studio format # First of all, you need a dataset in the YOLO format, which is structured like this:\ndataset ├── classes.txt ├── images │ ├── image1.png │ ├── image2.png │ └── ... └── labels ├── image1.txt ├── image2.txt └── ... Label studio cannot directly import a YOLO dataset that is labeled. It could import the images only, but for the labels to be loaded as well, we need to convert them to the label-studio json format.\nThis can be achieved by the label-studio-converter tool, which comes with label studio.\nThe converter needs 4 parameters:\n--input: The path to the dataset, i.e. in our case that would be /home/user/data/dataset\n--output: The path to the output file, i.e. /home/user/data/dataset/output.json. To make life easier, just put it into the dataset root directory.\n--image-root-url: Now this is the tricky one: The root URL is now a relative path. Don\u0026rsquo;t ask me why, but it needs to be relative to the parent of the root URL exported as our environment variable (as you will see later). And to make life even harder, it needs to be prefixed with /data/local-files/?d=. So in our case, the root URL would be /data/local-files/?d=dataset/images. Label studio will serve the images from the root URL and the images are located in the images folder of the dataset.\n--image-ext: The file extension of the images. In our case, that would be .png.\nAnd to assemble the full command:\nlabel-studio-converter import yolo -i /home/user/data/dataset -o /home/user/data/dataset/output.json --image-root-url \u0026#34;/data/local-files/?d=dataset/images\u0026#34; --image-ext .png This will write two files into the dataset directory: output.json and output.label_config.xml. The latter one contains the label format that we need to import the dataset into label studio.\n2. Export environment variables # To label local images, label-studio needs to be launched with two environment variables set. The first one enables local file serving and the second one sets the document root.\nThe document root must be an absolute path and the direct parent of the dataset directory. Example: If the root URL is /home/user/data then the dataset, itself containing an image and label folder, must reside in e.g. /home/user/data/dataset.\nexport LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true export LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/home/user/data To make this easier, I put this into a small bash script that exports the variables and launches label studio:\n#!/bin/bash export LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true export LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/home/user/data label-studio Just do a chmod +x on the script and you can start label studio with ./label-studio.sh.\n3. Create a new project in label studio # Open label-studio and create a new project.\nGive your project some name. Paste the content of the output.label_config.xml file into the label config field. Go to the settings and add a Cloud Storage, choosing Local Storage as the type. Set the storage root URL as an absolute path to the root of your dataset, i.e. /home/user/data/dataset. Now, import the output.json file into the project. You should now have the labels and images loaded into label studio.\n4. Label the images # Your project should now be ready to correct the labels. You can now start labeling the images. Once you are done, you can export the labels using the export button back into the YOLO format. This will produce a zip file containing the images and the labels. Unzip it and you are back where you started just with hopefully better labels.\nI hope this helps someone. If you have any questions, feel free to contact me.\n","date":"10 November 2023","externalUrl":null,"permalink":"/posts/label_studio_yolo/","section":"","summary":"A guide on how to set up Label Studio to correct pre-annotated images of a YOLO dataset locally.","title":"Labeling images with Label Studio","type":"posts"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/supervised-learning/","section":"Tags","summary":"","title":"Supervised-Learning","type":"tags"},{"content":"","date":"10 November 2023","externalUrl":null,"permalink":"/tags/yolo/","section":"Tags","summary":"","title":"Yolo","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/2023/","section":"Tags","summary":"","title":"2023","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/amapa/","section":"Tags","summary":"","title":"Amapa","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/brazil/","section":"Tags","summary":"","title":"Brazil","type":"tags"},{"content":"Recently, I had the privilege of joining Jan Benda on a research expedition to dive head first into the realm of electric fish in the Amazonian Rainforest.\nOver the course of nine days at a remote field station, we were immersed in the challenges and wonders of this unique ecosystem, all in pursuit of unraveling the mysteries of these aquatic creatures.\nA Pictorial Journey # Let\u0026rsquo;s start with some visuals to set the scene:\nWhy Electric Fish? # In the vast expanse of the Neotropics, electric fish are ubiquitous. While most people have heard of the electric eel, there are numerous lesser-known species that are small, nocturnal, and rarely observed without specialized equipment. These \u0026lsquo;weakly\u0026rsquo; electric fish utilize their electric organs for communication, navigation, and hunting in the Amazonian Rainforest waters. While they serve as a popular model in Neurophysiology, their ecological roles, natural behaviors, and communication methods remain largely enigmatic. This is where our journey begins.\nGymnorhamphichthys, a pulse-type weakly electric fish that inhabits the sand banks of Amazonian rivers. The Research Objective # Wave-type electric fish, comprising many species, are unique in that they constantly emit an electric field. We can measure this field using electrodes and amplifiers. The field of each fish oscillates at a unique frequency, allowing us to distinguish them. Our aim is to take this a step further: employing electrode grids to measure the electric field on a plane and reconstruct the movements, communication, and even the mating behaviors of multiple individuals in their natural habitat. While similar work has been done before, we aim to (1) enhance the resolution from the previous 64 to 256 electrodes and (2) make the system portable, modular, and robust. During this field trip, we tested the first prototype of this system.\nThe Expedition to the Field Station # Reaching our field station was no easy feat. We embarked on a journey that took us from Frankfurt to Lisbon and then to Belem, a city in northern Brazil. From Belem, we hopped on a small plane to Macapa, the capital of the state of Amapa. From there, we were graciously driven to Porto Grande by Christoph Jaster, head of the local park administration, where we met our boatsman, Junior, who would transport us to the field station. After two hours in the car and another two hours on the river, we arrived at the southernmost tip of the FLONA do Amapa, a national park nestled within the Amazonian Rainforest.\nThe field station, a small house covered by the dense canopy of the Amazonian Rainforest. The field station itself, perched at the confluence of the Rio Falsino and the Rio Araguari, was a humble abode with a small kitchen, bathrooms, and space for hammocks. In the evenings, a generator powered the lights, and we even had satellite internet. However, most of the time, we were serenaded by the sounds of the rainforest, the chirping of birds, and the distant howling of monkeys.\nA Day in the Life at the Field Station # Our days began with the early rising sun at around 6 AM. After a quick breakfast, the mornings were devoted to setting up, fixing, or debugging our equipment. In the afternoons, Junior would ferry us to the river for fieldwork, where we tested our recording hardware.\nJan and me taking a first look at new data on our favorite riverbank. Navigating the river\u0026rsquo;s shallows and rapids was exhilarating, especially since our visit coincided with the dry season, causing the water levels to drop noticeably. Despite the challenges, Junior skillfully guided our boat, and we never found ourselves stranded or pushing the boat through shallow waters. Maneuvering through the river\u0026rsquo;s rapids proved to be an enjoyable part of our daily routine.\nOn the lookout for big rocks beneath the surface. The sweltering midday heat often left us lethargic, prompting us to seek respite in our hammocks for a brief siesta or to \u0026ldquo;cool off\u0026rdquo; in the 32°C river waters. These moments also offered the opportunity to wash and dry our clothes in the sun within a few hours.\nMost evenings, we returned from the river with new challenges to tackle, leading us to spend the nights fixing and debugging our equipment. Telma, our cook, prepared meals for us and provided much-needed relief from cooking duties. After dinner, our evenings were dedicated to data analysis and equipment maintenance. Falling asleep in our sweaty hammocks, enveloped by the sounds of the rainforest, was a welcome reprieve after a long day\u0026rsquo;s work.\nEquipment and Data Collection # Our expedition was equipped with two main categories of hardware:\nFish Finders # At its simplest, a fish finder can be an audio amplifier. However, instead of plugging in a guitar or microphone, we connected our electrodes. This allowed us to audibly detect the electric organ discharges of the fish. While mere audio signals sufficed for locating the fish, Jan is in the process of developing a Smart Fish Finder with a screen capable of plotting fish waveforms. This advancement will enable us to discern the genus of the fish without visual confirmation.\nJan using his fish finder to record a Gymnorhamphichthys in the sand. Recording Hardware # Once we identified promising recording locations, we deployed our logging devices. Our goal is to create a platform of affordable, waterproof loggers constructed from readily available components. The most exciting aspect? These loggers are modular and adaptable for various recording purposes:\nLogger Configuration # Each logger consists of a microcontroller, a battery, an SD card, and currently, a temperature sensor. It can receive input from up to 16 electrodes, with recording duration limited only by the SD card\u0026rsquo;s capacity and battery life. In theory, they could record data for months, and deploying a network of loggers over a large area could capture migratory patterns of fish. This is the configuration we tested the most on during our field trip.\nGrid Configuration # Our plan is to employ these same loggers, with the addition of a cable for power, arranged side by side to form an electrode grid. With 16 electrodes, the placement of 16 loggers would result in a 256-electrode grid. This setup would enable us to record the electric field on a plane, facilitating the reconstruction of interactions and communication among multiple individuals. However, after five days of rigorous testing and debugging, we discovered that the current circuit boards introduced interference when devices were placed closely together. Thus, we will need to redesign and retest the circuit boards in the future.\nWildlife and Biodiversity # In addition to our electric fish research, our time in the rainforest afforded us the opportunity to observe a myriad of wildlife. We encountered a variety of avian species, including toucans, parrots, macaws, hummingbirds, and more. Our journey also brought us in contact with numerous insects, captivating moths and butterflies, reptiles, and amphibians.\nMost of the fish we encountered were small catfish, as the electric fish we sought tend to hide in crevices during the day. Nevertheless, we did have a few remarkable encounters, including Eigenmannia at night and an Archolaemus, which I managed to capture on video with my phone as it nestled between two rocks. This endeavor resulted in an amusing photo of me attempting to capture a good shot of the fish.\nObtaining a video was a source of great joy, given the elusive nature of these fish. Communicating about them using only data points on a screen can be challenging, so having a video was invaluable.\nWe also encountered a specimen of Gymnorhamphichthys, a pulse-type weakly electric fish that buries itself in the sand on riverbanks. Without our electrodes, locating them would have been exceedingly difficult, but with their help, we pinpointed two of them and even recorded a video of one. The background noise you hear is the electric organ discharge, which our amplifiers convert into sound.\nHowever, one electric fish appeared to shadow us wherever we ventured: Electrophorus, the electric eel. On our first day in the field, as we explored a small tributary of the Rio Falsino, we encountered four eels, each about 1.5 meters in length, serenely lounging in shallow water.\nIn the ensuing days, similar scenes greeted us each time we approached areas where smaller streams met the main channel. We also detected them (not visually, but via our fish finders) while working in the middle of the river. Picture standing chest-deep in the middle of a river, immersed in your work, and suddenly hearing the distinct \u0026rsquo;toc, toc, toc\u0026rsquo; of an approaching electric eel. When we heard them on the amplifiers, we knew they were just meters away, prompting us to swiftly exit the water and wait for them to pass. We also learned that they frequently traversed the swimming area at the station, as we had recorded them there before. Over time, we stopped carrying the fish finders when we wanted to relax and cool off, leaving the door open for future underwater encounters.\nI must confess that I had underestimated these creatures. I once naively assumed them to be \u0026lsquo;simple,\u0026rsquo; solitary hunters that relied on their electric organ to stun prey. However, as we were fortunate to observe, they are highly social animals, with documented instances of group hunting. I am eager to see what we can uncover about them in the future.\nAn electric eel passing by our recording electrodes on a tributary of the Rio Falsino. Challenges and Lessons Learned # Our expedition was not without its share of challenges, some expected and others not. Yet, we garnered valuable lessons and insights from them, fueling our enthusiasm to return to the field and test our new equipment.\nThe most significant lesson we took away was that no matter how extensively you test your equipment in the lab, field conditions will inevitably present new issues. Paradoxically, these challenges are opportunities to enhance the robustness of your equipment. Hence, this field trip was indispensable, as it exposed problems we would not have encountered in a controlled environment. We are elated to have embarked on this journey and eager to return.\nData Analysis and the Road Ahead # Currently, we are just starting analyzing the data we gathered during this expedition. Our exploration is just beginning, and we are thrilled about the potential discoveries that await us. We will keep you updated on our progress. In the meantime, here are some preliminary findings:\nThe first figure displays the pulse waveform of one of the Gymnorhamphichthys we recorded. The top panel shows the electric organ discharge recording, while the bottom panel illustrates the waveform of the pulses.\nThe second figure showcases a recording from one of our loggers, revealing at least four individuals, with three in the lower frequency range, potentially Sternopygi. The fourth individual operates in the higher frequency range.\nAcknowledgements # First and foremost, I would like to express my deepest gratitude to Jan Benda for extending the invitation to join him on this remarkable field trip. It was an invaluable opportunity to work with him and learn from his expertise.\nThis field trip would not have been possible without the generous assistance of many individuals. The logistical challenges of such an expedition were more significant than we anticipated, and we are incredibly thankful for the support we received.\nDavid de Santana played a pivotal role in connecting us with Christoph Jaster. Without this connection, we would have remained unaware of the existence of the FLONA do Amapa field station. We are sincerely appreciative of his contribution.\nChristoph Jaster diligently managed the permits and logistics from Macapa to the field station. His assistance was indispensable, particularly when our equipment was lost in transit. Furthermore, his kindness was evident when he drove us to the airport at 1 AM when our departure was imminent.\nOur heartfelt thanks go to Telma, our cook at the field station. Her delicious meals and unwavering kindness were a tremendous relief, sparing us the task of preparing our own food. We are deeply grateful for her support.\nJunior proved to be an exceptional boatsman. Communicating through gestures, a few words, and Google Translate, we not only enjoyed our time with him but also spent a memorable night under the stars in our hammocks along the Araguari River bank. We hope to have the privilege of working with him again in\nGroup picture of the overnighter on the river bank. From left to right: Me, Jan, and Junior. ","date":"7 November 2023","externalUrl":null,"permalink":"/posts/brasil_2023/","section":"","summary":"A small report on a field trip to the Amazonian Rainforest in Amapa, Brazil in 2023","title":"Exploring Electric Fish in the Rainforest of Brazil","type":"posts"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/field-trip/","section":"Tags","summary":"","title":"Field Trip","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/rainforest/","section":"Tags","summary":"","title":"Rainforest","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/research/","section":"Tags","summary":"","title":"Research","type":"tags"},{"content":"","date":"7 November 2023","externalUrl":null,"permalink":"/tags/science/","section":"Tags","summary":"","title":"Science","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-analysis/","section":"Tags","summary":"","title":"Data Analysis","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"Data Engineering","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-science-stack/","section":"Tags","summary":"","title":"Data Science Stack","type":"tags"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tags/data-visualization/","section":"Tags","summary":"","title":"Data Visualization","type":"tags"},{"content":"The Python ecosystem is huge and it is growing every day. It is hard to keep up with all the new libraries and tools. In this post I will share my tech stack for data science in 2023. I will update this post every year to document my plunge into this never ending rabbit hole.\nDisclaimer: Everything below is my opinion. If I get something wrong, please let me know. I don\u0026rsquo;t use all of these tools in every project and I don\u0026rsquo;t list all tools that I use here. I am merely providing a list of tools that I particularly enjoy using and that I think are worth checking out as I am writing this post.\nWhy Python? # Pythion is a general purpose programming language that is used for a wide variety of applications. It is a great language for data science because it is easy to learn, has a huge ecosystem of libraries and tools and it is free and open source. And beyond data science, it can be used for almost anything else.\nTerminal and shell # The terminal is where I spend half of my working day. Ideally, it should be fast, functional and pretty. And most importantly, it should be simple. For this reason, I switched from Alacritty to Kitty:\nKitty - Kitty is a fast, featureful, GPU based terminal emulator. It is fast, has great support for unicode and is highly customizable. It is my go to terminal emulator. It is also cross platform and works on Windows, Linux and macOS. Compared to Alacritty, it requires a minimal amount of configuration and is easy to set up. It comes with sane keybindings out of the box and even has build-in support for ligatures, emojis, images and even tiling! No need to configure tmux. The same goes for the shell:\nZsh - As opposed to Bash, Zsh is a shell designed for interactive use. It is a great shell with a huge ecosystem of plugins and themes. I use it with starship as a prompt and vim keybindings. This way I always now where I am, which environment I am in and what git branch I am on. Text editor # The text editor is where I spend the other half of my working day. After trying a lot of different editors, I have settled on two editors that I use for different purposes:\nVisual Studio Code - Visual Studio Code is a free code editor made by Microsoft for Windows, Linux and macOS. It is a bit slow but a breeze to set up and configure and settings sync between devices. It has a huge ecosystem of extensions and is my go to editor for almost everything. Because it is made by Microsoft, it integrates perfectly with GitHub Copilot, a GPT-4 powered AI that helps to make repetitive tasks easier. It is a great tool for writing code.\nNeovim - Neovim is a fork of Vim aiming to improve user experience and plugins. It is a bit harder to set up and configure but it is blazing fast and has a huge ecosystem of plugins. I use it for quick edits and for editing files on remote servers. I used it exclusively for a while but eventually broke my config during a time when I was too busy to fix it.\nNow that we can write code, we need to set up a virtual environment to run it. In my opinion, virtual environments are a must for any Python project. They allow you to create isolated environments for each project and make sure that the correct versions of your dependencies are installed. This is especially important for data science projects because they often require specific versions of Python and its dependencies. It also prevents your system from getting cluttered with dependencies that you don\u0026rsquo;t need. And since the environment can be frozen and shared with others, it makes your project reproducible.\nManging a Python environment # Let\u0026rsquo;s say your system uses Python 3.9 but the package you want to use requires Python 3.11. This is where pyenv comes in:\nPyenv - Pyenv lets you easily switch between multiple versions of Python, just like Anaconda but without Anaconda! It is a great tool for managing Python versions and virtual environments. For smaller projects, pyenv-virtualenv is a great tool that lets you create virtual environments that are all stored in one place. For larger projects, I prefer pythons build in venv module. If you have used Pythons build-in venv module, you know that it is a bit of a hassle to always source .venv/bin/activate. This is where direnv comes in:\nDirenv - Direnv is an environment switcher for the shell. It knows how to hook into bash, zsh, etc. to load or unload environment variables depending on the current directory. This is a great tool for managing environment variables and secrets. I use it to automatically activate and deactivate virtual environments. Pyenv can do this out of the box but only for pyenv virtual environments. Direnv can do this for any virtual environment. Now that we have an environment, we can start setting up a project.\nManaging a Python project # Version control is integral to any software project. It allows you to keep track of changes to your code and makes it easy to collaborate with others. This is where git and GitHub come in:\nGit - Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. It is the de facto standard for version control and is a must for any software project.\nGithub - GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.\nNow that we set up a git repository, we can install a pre-commit hook, that runs every time we push changes to the remote repository:\nPre-commit - Pre-commit is a framework for managing and maintaining pre-commit hooks. Pre-commit hooks are scripts that run before a commit is made. They can be used to check for formatting, linting, security issues, etc. I use it to check for formatting, linting and security issues. It is a great tool for maintaining code quality: If a pre-commit hook fails, the commit is aborted and the error is shown to the developer so that it can be fixed. This keeps remote repositories clean and makes sure that all code is formatted, linted and secure. The next tool is a great addition to pre-commit: Keeping the code clean # Black - Black is the uncompromising Python code formatter. It is a great tool for keeping code clean and consistent. isort - A tool to sort package imports consistently. ruff - The fastest python formatter that also sorts imports, this is my current go-to tool. Pylint - Pylint is a tool that checks for errors in Python code, tries to enforce a coding standard. Pytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. If you build a Python package, the next one is particularly useful:\nPoetry - Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. It is a great tool for managing dependencies and packaging because it locks versions of your depencencies to their exact commit hash. Combined with the correct Python version and virtual environment, this makes your project reproducible, which is a must, particularly for data science projects. Data science libraries # Now that we have set up a project, we can start writing code. The following libraries are the ones that I use most often to analyze data, train machine learning models and visualize results:\nNumpy - NumPy is the fundamental package for scientific computing in Python. It is a great library for working with arrays and matrices. It is the foundation of the scientific Python ecosystem and is used by almost every other library.\nPandas - Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. It is a great library for working with tabular data.\nScipy - SciPy is a Python-based ecosystem of open-source software for mathematics, science, and engineering. It is a great library for scientific computing and includes algorithms for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and many more.\nMatplotlib - Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. It is a great library for data visualization. It provides fine grained control over every aspect of a figure and is highly customizable.\nPydantic - Pydantic is a library for data validation and settings management based on Python type hints. It is a great library for validating data and settings. I use it to create data models for my data science projects.\nRich - Rich is a Python library for rich text and beautiful formatting in the terminal. It is a much nicer way to display logs, data and progress bars compared to e.g. tqdm. It is also the progress bar that pip uses when you install something.\nMachine learning tools # Scikit-learn - Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN. It is a great library for machine learning.\nPytorch - PyTorch is an open source library for deep learning and GPU accelerated computing. It is pain-free to install compared to TensorFlow and has a great API.\nTensorflow - Another deep learning framework that comes with Keras, a high level API that makes the usage of pretrained models as well as the creation of coustom models very fast.\nThe following set of packages are training frameworks, that mostly build on top of pytorch to abstract a lot of the boiler plate code and make model training and evalutation easier.\nFastAI - The library is a bit dated but the website provides amazing courses on deep learning. Keras - A high level API that works with Tensorflow, Pytorch and JAX. Pytorch Lightning - This is framework to make pytorch models as easy as fitting sklearn models. SuperGradients - The new kid on the block by DECI-AI. Home of the current state of the art object detection model YOLO-NAS. If a project gets bigger and other people start using it, the following tools are great for documentation:\nDocumenting and sharing # Jupyter - Jupyter is a free, open-source, interactive web tool known as a computational notebook, which researchers can use to combine software code, computational output, explanatory text and multimedia resources in a single document. Still, I try to avoid coding in Jupyter notebooks as much as possible but it is a great tool for sharing and documenting exploratory data analysis.\nMkdocs - MkDocs is a fast, simple and downright gorgeous static site generator that\u0026rsquo;s geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. MkDocs builds completely static HTML sites that you can host on GitHub pages, Amazon S3, or anywhere else you choose.\nPdoc - Like mkdocs, pdoc is a documentation generator for Python modules. Unlike mkdocs, it is specifically designed to be used with docstrings in Python code: pdoc automatically generates documentation from type annotations and docstrings. It is also much faster than mkdocs and is best used for API documentation.\nReporting # LaTeX - LaTeX is a typesetting system that includes features designed for the production of technical and scientific documentation. It is my favourite tool for writing reports, posters and presentations. It is a bit hard to learn but once you get the hang of it, it is a breeze to use. And the best part is that it is all in plain text so it works great with version control. Conclusion # As this is the first version of this post, It does include some tools that are obvious choices for data science in python. But I still think they are worth mentioning because they are so foundational. As I update this post every year, I will remove tools that are no longer relevant and add new tools that I think are worth checking out. If you have any suggestions, please let me know.\n","date":"6 November 2023","externalUrl":null,"permalink":"/posts/datascience-stack/","section":"","summary":"A collection of libaries and tools that I use to analyze data with Python in 2023.","title":"My data science tools in 2023","type":"posts"},{"content":"","date":"1 May 2023","externalUrl":null,"permalink":"/tags/arduino/","section":"Tags","summary":"","title":"Arduino","type":"tags"},{"content":"","date":"1 May 2023","externalUrl":null,"permalink":"/tags/hardware/","section":"Tags","summary":"","title":"Hardware","type":"tags"},{"content":"","date":"1 May 2023","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"If you\u0026rsquo;ve worked with Arduinos before, you know that beeing constrained to the Arduino IDE can be quite limiting. PlatformIO is a cross-platform, cross-architecture, and multiple framework tool that can help with this. It is designed to make hardware development more comfortable and more efficient.\nPlatformIO is typically advertised as a VSCode plugin, which turns VSCode into an Arduino IDE-like platform. However, there is a lesser-known feature that developers can take advantage of - the simple command-line toolbox. In this blog post, we will walk you through how to install and use PlatformIO via the command line.\nTo begin, PlatformIO is a Python package, so it can be installed from PyPI, including within a virtual environment. However, it is highly recommended to use PlatformIO\u0026rsquo;s installation script instead. Here\u0026rsquo;s how you can install PlatformIO using the installation script:\ncurl -fsSL https://raw.githubusercontent.com/platformio/platformio-core-installer/master/get-platformio.py -o get-platformio.py python3 get-platformio.py During installation, PlatformIO creates its virtual environment, which is beneficial since it doesn\u0026rsquo;t interfere with system packages. However, the standard shell doesn\u0026rsquo;t recognize the PlatformIO executables, so we need to create symbolic links. You can do that using the following commands:\nln -s ~/.platformio/penv/bin/platformio ~/.local/bin/platformio ln -s ~/.platformio/penv/bin/pio ~/.local/bin/pio ln -s ~/.platformio/penv/bin/piodebuggdb ~/.local/bin/piodebuggdb On a Linux system, you also need to add udev rules. This snippet downloads the rules, places them in the appropriate location, reloads the udev service, and adds the current user to the relevant groups:\ncurl -fsSL https://raw.githubusercontent.com/platformio/platformio-core/develop/platformio/assets/system/99-platformio-udev.rules | sudo tee /etc/udev/rules.d/99-platformio-udev.rules To restart udev on debian-based distributions, run:\nsudo service udev restart sudo usermod -a -G dialout $USER sudo usermod -a -G plugdev $USER On arch-based distributions, use the following instead:\nsudo udevadm control --reload-rules sudo udevadm trigger sudo usermod -a -G uucp $USER sudo usermod -a -G lock $USER Log out and log back in for the changes to take effect.\nBy default, PlatformIO enables telemetry. However, you can disable it by running:\npio settings set enable_telemetry No pio settings get Next, you can initialize pio in your directory by creating an example project directory, for example, mkdir pio_test \u0026amp;\u0026amp; cd pio_test. Then, list all the available boards (platforms) by running pio boards. Choose your board and run:\npio project init --ide vim --board teensy41 Now you can edit the source code using your editor of choice. To test if the code compiles, you can simply run:\npio run To flash an attached board, add the following:\npio run --target upload For convenience, I personally created shell aliases comp for compilation and flash to upload. If you let your device access the serial port to print some debug information, you can monitor this by running:\npio device In conclusion, PlatformIO is a powerful tool for hardware development that provides a flexible and efficient workflow for software developers working with hardware. With its support for multiple platforms, frameworks, and libraries, along with a command-line interface and integrated debugging, PlatformIO is an excellent choice for anyone looking to streamline their hardware development workflow.\nPhoto by Vishnu Mohanan on Unsplash\n","date":"1 May 2023","externalUrl":null,"permalink":"/posts/platformio-introduction/","section":"","summary":"An introduction to PlatformIO, a tool that makes software development for microcontrollers easier.","title":"PlatformIO: Hardware dev from the command line","type":"posts"},{"content":"This is a small selection of projects and activities that I currently pursue in my free time (if there is any).\nLinux and open source software # I am a huge fan of Linux and open source software. I got into Linux when I started working in the Neuroethology lab at the University of Tuebingen, where everybody was using it. The idea of a free and open operating system that is not only highly customizable but also very secure instantly fascinated me. Linux has been my main operating system for approximately 3 years now. I am currently running Arch Linux with the Hyprland window manager on both my personal laptop and work PC. I keep a dotfiles repository on GitHub where I store my configuration files and scripts.\nweygoldt/dots My personal dotfiles. Python 0 0 Scuba diving # I am a certified PADI Advanced Open Water Diver. Diving always fascinated me but doing my Bachelors thesis in a marine biology lab provided the spark to finally get certified. At this time I have been diving in Germany, Indonesia and the Mediterranean Sea. I am really excited to continue my education as a diver, a skill that I\u0026rsquo;m sure will come in handy for my future research. I am particulary drawn towards the idea of becoming a technical- and cave diver to be able to explore challenging environments. In 2023, I got a first chance to combine this skill with my work during a four-week stay at the STARESO marine research station where I assisted PhD-students during sample collection dives.\nScorpion-fish catching in the Mediterranean sea. Photo courtesy of Lena Wesenberg To plan and log my dives, I use Subsurface, an open source dive log software written by no other than the creator of the Linux kernel, Linus Torvalds.\nHiking and everything outdoors # Before I started my studies I spent my weekends with photography trips, mostly hiking in the mountains. Nowadays, I don\u0026rsquo;t have as much time for photography as I used to, but I still enjoy hiking, even if I rarely take my camera. The image below is one of my favorite pictures from that time, a self-portrait taken on a bivouac in the Alps.\nA self-portrait taken on a bivouac in the alps close to the Grimsel pass in Switzerland. For more images, check out my Instagram account or my Youtube channel for some videos of my trips.\n","date":"25 February 2023","externalUrl":null,"permalink":"/personal/","section":"What I do for fun","summary":"","title":"What I do for fun","type":"personal"},{"content":"","date":"8 December 2022","externalUrl":null,"permalink":"/tags/caimg/","section":"Tags","summary":"","title":"Caimg","type":"tags"},{"content":"Experimental research has shown that zebrafish are less likely to perceive motion when the only moving element is color. Instead, it is the differences in brightness between moving stimuli that trigger a response (Orger and Baier, 2005). As part of a lab rotation project, we investigated how this behavior manifests in the optic tectum, the primary visual processing center in fish.\nTo achieve this, we employed two-photon calcium imaging to record the activity of direction selective neurons in the optic tectum of zebrafish larvae. Additionally, we simultaneously measured the optokinetic response, a behavioral indicator of motion perception.\nThe resulting dataset consisted of a z-stack of fluorescence microscopy images and the eye movements of the zebrafish embryo. After segmenting the cells with suite2p we analyzed the calcium activity (i.e., the luminance of each cell) and the eye velocities with respect to the stimulus that was shown to the fish.\nThe ensuing graphs illustrate a comparable pattern between the calcium activities (neural signal) and eye velocities (behavioral output) for the same stimulation conditions. Calcium acitivty in the Zebrafish optic tectum when stimulated with different levels of chromatic and achromatic contrasts. Behavioral output measured in the eye velocities during the optokinetic response shows a similar pattern compared to the neural activity. Our analysis suggested that the direction selective neurons in the optic tectum are likely colorblind, as they primarily respond to variations in brightness rather than color distinctions. As a result of this experiment, we have produced not only a poster but also a more comprehensive report, both of which can be found in the GitHub repository provided below.\nweygoldt/colorblind-directioncells A lab rotation with the systems neuroscience lab, University of Tuebingen, where we tested whether color contrast contributed to encoding motion in direction selective cells of the zebrafish optic tectum. Jupyter Notebook 0 0 Image by Zeiss\n","date":"8 December 2022","externalUrl":null,"permalink":"/posts/colorbling_direction_cells/","section":"","summary":"Some neurons in thet zebrafish brain respond to movement direction. We recorded and analyzed the activity of these neurons and showed, that these cells probably rely on luminance alone to perceive motion.","title":"Colorblind direction cells","type":"posts"},{"content":"","date":"8 December 2022","externalUrl":null,"permalink":"/tags/neuroscience/","section":"Tags","summary":"","title":"Neuroscience","type":"tags"},{"content":"","date":"8 December 2022","externalUrl":null,"permalink":"/tags/zebrafish/","section":"Tags","summary":"","title":"Zebrafish","type":"tags"},{"content":"","date":"7 November 2022","externalUrl":null,"permalink":"/tags/covariance/","section":"Tags","summary":"","title":"Covariance","type":"tags"},{"content":"While sifting through a two-week continuous recording of electric fish in their natural habitat, I observed synchronous frequency modulations on a scale of seconds to two ten minutes between two fish. To detect these modulations, I developed a covariance-based detector, which I then used to detect these events on recordings. The following plot shows some examples of detected modulations: Synchronous rises of the EODf betweeen two individuals recorded in freely interacting fish in their natural habitat. To detect spatio-temporal interactions, particularly when fish chased each other, I build a cost function that peaks when (1) fish swim in the same trajectories relative to each other, (2) velocities of both fish increase, and (3) fish accelerate rapidly. By analyzing the estimated positions of fish over time, I demonstrated that those involved in frequency duets approach one another following the initiation of their \u0026ldquo;choir.\u0026rdquo; I also rendered videos depicting some of these interactions: Fish moving as data points on an electrode grid and their frequencies changing on the right-hand side.\nThe resulting output from this effort was displayed as a poster at the 2022 International Conference of Neuroethology (ICN). This poster can be accessed through the link provided in the GitHub repository below.\nweygoldt/synchronous-modulations Working directory including scripts for the analysis of synchronised EOD frequency modulations in freely interacting Brown Ghost Knifefish recorded in Colombia. Jupyter Notebook 0 0 ","date":"7 November 2022","externalUrl":null,"permalink":"/posts/electric_duet/","section":"","summary":"Some electric fish synchronously modulate their frequencies in duets. I detected these events using a custom covariance-based event detector on frequency estimates from spectrograms.","title":"Electric duet","type":"posts"},{"content":" ","date":"13 June 2022","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]